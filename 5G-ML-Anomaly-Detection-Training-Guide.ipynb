{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0358915",
   "metadata": {},
   "source": [
    "# 5Gç¶²è·¯MLç•°å¸¸åµæ¸¬æ¨¡å‹è¨“ç·´å®Œæ•´æŒ‡å—\n",
    "\n",
    "## æ¦‚è¿°\n",
    "\n",
    "æœ¬ç­†è¨˜è©³ç´°èªªæ˜å¦‚ä½•åœ¨5Gç¶²è·¯ç’°å¢ƒä¸­è¨“ç·´æ©Ÿå™¨å­¸ç¿’ç•°å¸¸åµæ¸¬æ¨¡å‹ï¼ŒåŒ…å«å®Œæ•´çš„è¨“ç·´æµç¨‹ã€ç‰¹å¾µå·¥ç¨‹ã€æ¨¡å‹é¸æ“‡ã€è©•ä¼°æ–¹æ³•ç­‰ã€‚\n",
    "\n",
    "### ç³»çµ±æ¶æ§‹\n",
    "- **MLAnomalyDetector**: æ ¸å¿ƒæ©Ÿå™¨å­¸ç¿’ç•°å¸¸åµæ¸¬å™¨\n",
    "- **MLFeatureExtractor**: ç‰¹å¾µæå–å™¨\n",
    "- **RealTimeAnomalyDetector**: å¯¦æ™‚ç•°å¸¸åµæ¸¬ç³»çµ±\n",
    "- **TestSelector**: æ•´åˆæ¸¬è©¦ç³»çµ±\n",
    "\n",
    "### æ”¯æ´çš„MLæ¨¡å‹\n",
    "1. **Isolation Forest** (ç„¡ç›£ç£) - æ¨è–¦ç”¨æ–¼æ¸¬è©¦\n",
    "2. **One-Class SVM** (ç„¡ç›£ç£)\n",
    "3. **Random Forest** (ç›£ç£å¼) - éœ€è¦æ¨™è¨˜æ•¸æ“š\n",
    "4. **DBSCAN** (ç„¡ç›£ç£èšé¡)\n",
    "\n",
    "### æ‡‰ç”¨å ´æ™¯\n",
    "- 5G gNBéš¨æ©Ÿæ¥å…¥(RA)ç¨‹åºç•°å¸¸åµæ¸¬\n",
    "- ç¶²è·¯æ”»æ“Šæª¢æ¸¬\n",
    "- ä¿¡è™Ÿå¹²æ“¾æª¢æ¸¬\n",
    "- è¨­å‚™æ•…éšœè¨ºæ–·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3731e7",
   "metadata": {},
   "source": [
    "## 1. å°å…¥å¿…è¦çš„å‡½å¼åº«\n",
    "\n",
    "é¦–å…ˆå°å…¥æ©Ÿå™¨å­¸ç¿’è¨“ç·´æ‰€éœ€çš„æ‰€æœ‰Pythonå‡½å¼åº«ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072740ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥æ ¸å¿ƒæ©Ÿå™¨å­¸ç¿’å‡½å¼åº«\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å°å…¥scikit-learnæ©Ÿå™¨å­¸ç¿’æ¨¡å‹\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# å°å…¥å…¶ä»–å¿…è¦å‡½å¼åº«\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "\n",
    "# è¨­å®šé¡¯ç¤ºé¸é …\n",
    "plt.style.use('default')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰å¿…è¦å‡½å¼åº«å·²æˆåŠŸå°å…¥\")\n",
    "print(f\"NumPyç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"Pandasç‰ˆæœ¬: {pd.__version__}\")\n",
    "print(f\"Matplotlibç‰ˆæœ¬: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-learnç‰ˆæœ¬: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e31512",
   "metadata": {},
   "source": [
    "## 2. å»ºç«‹MLç•°å¸¸åµæ¸¬å™¨é¡åˆ¥\n",
    "\n",
    "`MLAnomalyDetector`æ˜¯æ ¸å¿ƒçš„æ©Ÿå™¨å­¸ç¿’ç•°å¸¸åµæ¸¬å™¨ï¼Œæ”¯æ´å¤šç¨®MLç®—æ³•ä¸¦æä¾›çµ±ä¸€çš„è¨“ç·´å’Œé æ¸¬ä»‹é¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8736325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLAnomalyDetector:\n",
    "    \"\"\"\n",
    "    æ©Ÿå™¨å­¸ç¿’ç•°å¸¸åµæ¸¬å™¨ - ä½¿ç”¨å¤šç¨®MLç®—æ³•é€²è¡Œ5Gç¶²è·¯ç•°å¸¸åµæ¸¬\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'isolation_forest'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–MLç•°å¸¸åµæ¸¬å™¨\n",
    "        \n",
    "        åƒæ•¸:\n",
    "            model_type: æ¨¡å‹é¡å‹ ('isolation_forest', 'one_class_svm', 'random_forest', 'dbscan')\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.is_trained = False\n",
    "        self.feature_names = []\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        # åˆå§‹åŒ–é¸å®šçš„æ¨¡å‹\n",
    "        self._initialize_model()\n",
    "        print(f\"âœ… MLAnomalyDetectoråˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹é¡å‹: {model_type}\")\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"æ ¹æ“šæ¨¡å‹é¡å‹åˆå§‹åŒ–å°æ‡‰çš„MLæ¨¡å‹å’Œæ•¸æ“šæ¨™æº–åŒ–å™¨\"\"\"\n",
    "        \n",
    "        if self.model_type == 'isolation_forest':\n",
    "            # Isolation Forest: åŸºæ–¼éš”é›¢çš„ç•°å¸¸åµæ¸¬\n",
    "            self.model = IsolationForest(\n",
    "                contamination=0.1,      # é æœŸç•°å¸¸æ¯”ä¾‹10%\n",
    "                random_state=42,\n",
    "                n_estimators=100,       # æ±ºç­–æ¨¹æ•¸é‡\n",
    "                max_samples='auto',     # æ¯æ£µæ¨¹çš„æ¨£æœ¬æ•¸\n",
    "                bootstrap=False\n",
    "            )\n",
    "            self.scaler = StandardScaler()\n",
    "            \n",
    "        elif self.model_type == 'one_class_svm':\n",
    "            # One-Class SVM: å–®é¡æ”¯æŒå‘é‡æ©Ÿ\n",
    "            self.model = OneClassSVM(\n",
    "                kernel='rbf',           # å¾‘å‘åŸºæ ¸å‡½æ•¸\n",
    "                gamma='scale',          # æ ¸å‡½æ•¸ä¿‚æ•¸\n",
    "                nu=0.1                  # ç•°å¸¸æ¯”ä¾‹ä¸Šç•Œ\n",
    "            )\n",
    "            self.scaler = StandardScaler()\n",
    "            \n",
    "        elif self.model_type == 'random_forest':\n",
    "            # Random Forest: éš¨æ©Ÿæ£®æ—åˆ†é¡å™¨ï¼ˆç›£ç£å­¸ç¿’ï¼‰\n",
    "            self.model = RandomForestClassifier(\n",
    "                n_estimators=100,       # æ±ºç­–æ¨¹æ•¸é‡\n",
    "                random_state=42,\n",
    "                class_weight='balanced', # å¹³è¡¡é¡åˆ¥æ¬Šé‡\n",
    "                max_depth=10,           # æ¨¹çš„æœ€å¤§æ·±åº¦\n",
    "                min_samples_split=5,    # ç¯€é»åˆ†å‰²æ‰€éœ€æœ€å°æ¨£æœ¬æ•¸\n",
    "                min_samples_leaf=2      # è‘‰ç¯€é»æœ€å°æ¨£æœ¬æ•¸\n",
    "            )\n",
    "            self.scaler = RobustScaler()  # å°ç•°å¸¸å€¼æ›´ç©©å¥\n",
    "            \n",
    "        elif self.model_type == 'dbscan':\n",
    "            # DBSCAN: åŸºæ–¼å¯†åº¦çš„èšé¡ç®—æ³•\n",
    "            self.model = DBSCAN(\n",
    "                eps=0.5,                # é„°åŸŸåŠå¾‘\n",
    "                min_samples=5,          # æ ¸å¿ƒé»æœ€å°æ¨£æœ¬æ•¸\n",
    "                metric='euclidean'      # è·é›¢åº¦é‡\n",
    "            )\n",
    "            self.scaler = StandardScaler()\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {self.model_type}\")\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"ç²å–æ¨¡å‹è©³ç´°è³‡è¨Š\"\"\"\n",
    "        return {\n",
    "            'model_type': self.model_type,\n",
    "            'is_trained': self.is_trained,\n",
    "            'feature_count': len(self.feature_names),\n",
    "            'feature_names': self.feature_names,\n",
    "            'threshold': self.threshold,\n",
    "            'model_params': self.model.get_params() if self.model else None\n",
    "        }\n",
    "\n",
    "# å‰µå»ºç¤ºä¾‹åµæ¸¬å™¨\n",
    "detector_example = MLAnomalyDetector('isolation_forest')\n",
    "print(f\"æ¨¡å‹è³‡è¨Š: {detector_example.get_model_info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06872a14",
   "metadata": {},
   "source": [
    "## 3. ç‰¹å¾µæå–èˆ‡æ•¸æ“šé è™•ç†\n",
    "\n",
    "ç‰¹å¾µæå–æ˜¯MLç•°å¸¸åµæ¸¬çš„é—œéµæ­¥é©Ÿï¼Œæˆ‘å€‘å¾5Gç¶²è·¯çš„RAçµ±è¨ˆæ•¸æ“šã€ä¿¡è™Ÿå¼·åº¦ã€æ™‚é–“æˆ³ç­‰åŸå§‹æ•¸æ“šä¸­æå–æœ‰æ„ç¾©çš„ç‰¹å¾µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFeatureExtractor:\n",
    "    \"\"\"\n",
    "    æ©Ÿå™¨å­¸ç¿’ç‰¹å¾µæå–å™¨ - å¾5Gç¶²è·¯æ•¸æ“šä¸­æå–ç•°å¸¸åµæ¸¬ç‰¹å¾µ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        \n",
    "    def extract_ra_features(self, ra_stats: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        å¾RAçµ±è¨ˆæ•¸æ“šä¸­æå–ç‰¹å¾µ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "            ra_stats: RAçµ±è¨ˆè³‡æ–™å­—å…¸\n",
    "            \n",
    "        è¿”å›:\n",
    "            æå–çš„RAç‰¹å¾µå­—å…¸\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # åŸºæœ¬çµ±è¨ˆç‰¹å¾µ\n",
    "        features['ra_success_rate'] = ra_stats.get('success_rate', 0)\n",
    "        features['ra_initiated'] = ra_stats.get('ra_initiated', 0)\n",
    "        features['ra_succeeded'] = ra_stats.get('ra_succeeded', 0)\n",
    "        features['ra_failed'] = ra_stats.get('failed_attempts', 0)\n",
    "        \n",
    "        # è¡ç”Ÿç‰¹å¾µ\n",
    "        total_ra = ra_stats.get('ra_initiated', 1)\n",
    "        features['ra_frequency'] = total_ra / 20  # å‡è¨­20ç§’æ¸¬è©¦æ™‚é–“\n",
    "        features['failure_rate'] = ra_stats.get('failed_attempts', 0) / max(total_ra, 1)\n",
    "        features['success_efficiency'] = ra_stats.get('ra_succeeded', 0) / max(total_ra, 1)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_timing_features(self, timestamps: List[float]) -> Dict:\n",
    "        \"\"\"\n",
    "        å¾æ™‚é–“æˆ³åºåˆ—ä¸­æå–æ™‚åºç‰¹å¾µ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "            timestamps: æ™‚é–“æˆ³åˆ—è¡¨\n",
    "            \n",
    "        è¿”å›:\n",
    "            æ™‚åºç‰¹å¾µå­—å…¸\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        if len(timestamps) < 2:\n",
    "            return {\n",
    "                'avg_interval': 0, 'interval_variance': 0, 'interval_std': 0,\n",
    "                'min_interval': 0, 'max_interval': 0, 'interval_range': 0\n",
    "            }\n",
    "        \n",
    "        # è¨ˆç®—æ™‚é–“é–“éš”\n",
    "        intervals = np.diff(timestamps)\n",
    "        \n",
    "        # çµ±è¨ˆç‰¹å¾µ\n",
    "        features['avg_interval'] = np.mean(intervals)\n",
    "        features['interval_variance'] = np.var(intervals)\n",
    "        features['interval_std'] = np.std(intervals)\n",
    "        features['min_interval'] = np.min(intervals)\n",
    "        features['max_interval'] = np.max(intervals)\n",
    "        features['interval_range'] = np.max(intervals) - np.min(intervals)\n",
    "        \n",
    "        # è¦å¾‹æ€§ç‰¹å¾µ\n",
    "        if len(intervals) > 3:\n",
    "            # è¨ˆç®—ç›¸é„°é–“éš”çš„è®ŠåŒ–ç‡\n",
    "            interval_changes = np.abs(np.diff(intervals))\n",
    "            features['interval_stability'] = 1 / (1 + np.mean(interval_changes))\n",
    "        else:\n",
    "            features['interval_stability'] = 1\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_signal_features(self, signal_data: List[float]) -> Dict:\n",
    "        \"\"\"\n",
    "        å¾ä¿¡è™Ÿå¼·åº¦æ•¸æ“šä¸­æå–ç‰¹å¾µ\n",
    "        \n",
    "        åƒæ•¸:\n",
    "            signal_data: ä¿¡è™Ÿå¼·åº¦åˆ—è¡¨\n",
    "            \n",
    "        è¿”å›:\n",
    "            ä¿¡è™Ÿç‰¹å¾µå­—å…¸\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        if not signal_data:\n",
    "            return {\n",
    "                'signal_mean': 0, 'signal_std': 0, 'signal_variance': 0,\n",
    "                'signal_min': 0, 'signal_max': 0, 'signal_range': 0,\n",
    "                'signal_change_rate': 0\n",
    "            }\n",
    "        \n",
    "        signal_array = np.array(signal_data)\n",
    "        \n",
    "        # åŸºæœ¬çµ±è¨ˆç‰¹å¾µ\n",
    "        features['signal_mean'] = np.mean(signal_array)\n",
    "        features['signal_std'] = np.std(signal_array)\n",
    "        features['signal_variance'] = np.var(signal_array)\n",
    "        features['signal_min'] = np.min(signal_array)\n",
    "        features['signal_max'] = np.max(signal_array)\n",
    "        features['signal_range'] = np.max(signal_array) - np.min(signal_array)\n",
    "        \n",
    "        # ä¿¡è™Ÿè®ŠåŒ–ç‰¹å¾µ\n",
    "        if len(signal_data) > 1:\n",
    "            signal_diff = np.diff(signal_array)\n",
    "            features['signal_change_rate'] = np.mean(np.abs(signal_diff))\n",
    "            features['signal_trend'] = np.corrcoef(range(len(signal_array)), signal_array)[0, 1]\n",
    "        else:\n",
    "            features['signal_change_rate'] = 0\n",
    "            features['signal_trend'] = 0\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def extract_comprehensive_features(self, test_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        æå–ç¶œåˆç‰¹å¾µå‘é‡\n",
    "        \n",
    "        åƒæ•¸:\n",
    "            test_data: æ¸¬è©¦æ•¸æ“šå­—å…¸\n",
    "            \n",
    "        è¿”å›:\n",
    "            å®Œæ•´çš„ç‰¹å¾µå­—å…¸\n",
    "        \"\"\"\n",
    "        all_features = {}\n",
    "        \n",
    "        # RAç¨‹åºç‰¹å¾µ\n",
    "        if 'ra_stats' in test_data:\n",
    "            ra_features = self.extract_ra_features(test_data['ra_stats'])\n",
    "            all_features.update({f'ra_{k}': v for k, v in ra_features.items()})\n",
    "        \n",
    "        # æ™‚åºç‰¹å¾µ\n",
    "        if 'timestamps' in test_data:\n",
    "            timing_features = self.extract_timing_features(test_data['timestamps'])\n",
    "            all_features.update({f'timing_{k}': v for k, v in timing_features.items()})\n",
    "        \n",
    "        # ä¿¡è™Ÿç‰¹å¾µ\n",
    "        if 'signal_data' in test_data:\n",
    "            signal_features = self.extract_signal_features(test_data['signal_data'])\n",
    "            all_features.update({f'signal_{k}': v for k, v in signal_features.items()})\n",
    "        \n",
    "        # ç’°å¢ƒèˆ‡é…ç½®ç‰¹å¾µ\n",
    "        all_features['test_duration'] = test_data.get('test_duration', 0)\n",
    "        all_features['attacker_present'] = 1 if test_data.get('attacker_active', False) else 0\n",
    "        all_features['airplane_mode_toggles'] = test_data.get('airplane_toggles', 0)\n",
    "        all_features['loop_number'] = test_data.get('loop_number', 0)\n",
    "        \n",
    "        # æ¸¬è©¦é¡å‹ç·¨ç¢¼\n",
    "        test_type = test_data.get('test_type', 'unknown')\n",
    "        all_features['test_type_cots_only'] = 1 if test_type == 'cots_only' else 0\n",
    "        all_features['test_type_standard'] = 1 if test_type == 'standard' else 0\n",
    "        all_features['test_type_attacker_priority'] = 1 if test_type == 'attacker_priority' else 0\n",
    "        \n",
    "        return all_features\n",
    "\n",
    "# å‰µå»ºç‰¹å¾µæå–å™¨ä¸¦ç¤ºç¯„ä½¿ç”¨\n",
    "extractor = MLFeatureExtractor()\n",
    "\n",
    "# ç¤ºä¾‹æ•¸æ“š\n",
    "sample_data = {\n",
    "    'ra_stats': {\n",
    "        'success_rate': 95.5,\n",
    "        'ra_initiated': 12,\n",
    "        'ra_succeeded': 11,\n",
    "        'failed_attempts': 1\n",
    "    },\n",
    "    'timestamps': [1.0, 2.5, 4.1, 6.2, 8.0],\n",
    "    'signal_data': [52.3, 54.1, 53.8, 55.2, 54.9],\n",
    "    'test_duration': 20,\n",
    "    'attacker_active': False,\n",
    "    'airplane_toggles': 4,\n",
    "    'loop_number': 1,\n",
    "    'test_type': 'cots_only'\n",
    "}\n",
    "\n",
    "# æå–ç‰¹å¾µ\n",
    "features = extractor.extract_comprehensive_features(sample_data)\n",
    "print(f\"âœ… æˆåŠŸæå– {len(features)} å€‹ç‰¹å¾µ\")\n",
    "print(\"ç‰¹å¾µç¯„ä¾‹:\")\n",
    "for i, (key, value) in enumerate(list(features.items())[:10]):\n",
    "    print(f\"  {key}: {value:.3f}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a47f15",
   "metadata": {},
   "source": [
    "## 4. åˆæˆè¨“ç·´æ•¸æ“šç”Ÿæˆ\n",
    "\n",
    "ç”±æ–¼çœŸå¯¦çš„5Gç¶²è·¯ç•°å¸¸æ•¸æ“šé›£ä»¥å–å¾—ï¼Œæˆ‘å€‘ç”Ÿæˆåˆæˆçš„è¨“ç·´æ•¸æ“šä¾†æ¨¡æ“¬æ­£å¸¸å’Œç•°å¸¸çš„ç¶²è·¯è¡Œç‚ºæ¨¡å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_training_data(num_normal=100, num_anomaly=30):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆåˆæˆçš„5Gç¶²è·¯è¨“ç·´æ•¸æ“š\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        num_normal: æ­£å¸¸æ¨£æœ¬æ•¸é‡\n",
    "        num_anomaly: ç•°å¸¸æ¨£æœ¬æ•¸é‡\n",
    "        \n",
    "    è¿”å›:\n",
    "        (normal_data, anomaly_data): æ­£å¸¸å’Œç•°å¸¸æ•¸æ“šåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    \n",
    "    normal_data = []\n",
    "    anomaly_data = []\n",
    "    \n",
    "    print(f\"ğŸ”„ æ­£åœ¨ç”Ÿæˆ {num_normal} å€‹æ­£å¸¸æ¨£æœ¬...\")\n",
    "    \n",
    "    # ç”Ÿæˆæ­£å¸¸è¡Œç‚ºæ•¸æ“š\n",
    "    for i in range(num_normal):\n",
    "        # æ­£å¸¸æƒ…æ³ï¼šé«˜æˆåŠŸç‡ã€ç©©å®šä¿¡è™Ÿã€è¦å¾‹æ™‚é–“é–“éš”\n",
    "        ra_stats = {\n",
    "            'success_rate': np.clip(np.random.normal(95, 5), 85, 100),  # é«˜æˆåŠŸç‡\n",
    "            'ra_initiated': np.random.randint(8, 15),                   # ä¸­ç­‰RAå˜—è©¦æ¬¡æ•¸\n",
    "            'ra_succeeded': lambda x: np.random.randint(max(1, int(x*0.85)), x+1),\n",
    "            'failed_attempts': np.random.randint(0, 3)                  # å°‘é‡å¤±æ•—\n",
    "        }\n",
    "        ra_stats['ra_succeeded'] = ra_stats['ra_succeeded'](ra_stats['ra_initiated'])\n",
    "        ra_stats['success_rate'] = (ra_stats['ra_succeeded'] / ra_stats['ra_initiated']) * 100\n",
    "        \n",
    "        # æ­£å¸¸æ™‚é–“æˆ³ï¼šç›¸å°è¦å¾‹çš„é–“éš”\n",
    "        base_interval = np.random.uniform(1.5, 3.0)\n",
    "        timestamps = []\n",
    "        current_time = 0\n",
    "        for _ in range(np.random.randint(8, 12)):\n",
    "            current_time += base_interval + np.random.normal(0, 0.3)  # å°çš„éš¨æ©Ÿè®Šå‹•\n",
    "            timestamps.append(current_time)\n",
    "        \n",
    "        # æ­£å¸¸ä¿¡è™Ÿï¼šç©©å®šä¸”åœ¨åˆç†ç¯„åœå…§\n",
    "        base_signal = np.random.uniform(48, 58)  # æ­£å¸¸ä¿¡è™Ÿå¼·åº¦ç¯„åœ\n",
    "        signal_data = [base_signal + np.random.normal(0, 2) for _ in range(10)]\n",
    "        \n",
    "        test_data = {\n",
    "            'ra_stats': ra_stats,\n",
    "            'test_duration': 20,\n",
    "            'attacker_active': False,  # æ­£å¸¸æƒ…æ³ç„¡æ”»æ“Š\n",
    "            'airplane_toggles': np.random.randint(3, 7),\n",
    "            'loop_number': i,\n",
    "            'test_type': 'normal',\n",
    "            'timestamps': timestamps,\n",
    "            'signal_data': signal_data\n",
    "        }\n",
    "        normal_data.append(test_data)\n",
    "    \n",
    "    print(f\"ğŸ”„ æ­£åœ¨ç”Ÿæˆ {num_anomaly} å€‹ç•°å¸¸æ¨£æœ¬...\")\n",
    "    \n",
    "    # ç”Ÿæˆç•°å¸¸è¡Œç‚ºæ•¸æ“š\n",
    "    for i in range(num_anomaly):\n",
    "        # ç•°å¸¸æƒ…æ³ï¼šä½æˆåŠŸç‡ã€ä¸ç©©å®šä¿¡è™Ÿã€ä¸è¦å¾‹æ™‚é–“é–“éš”\n",
    "        \n",
    "        # é¸æ“‡ç•°å¸¸é¡å‹\n",
    "        anomaly_type = np.random.choice(['attack', 'interference', 'hardware_failure'])\n",
    "        \n",
    "        if anomaly_type == 'attack':\n",
    "            # æ”»æ“Šå ´æ™¯ï¼šå¤§é‡RAå˜—è©¦ã€è¼ƒä½æˆåŠŸç‡\n",
    "            ra_stats = {\n",
    "                'success_rate': np.clip(np.random.normal(60, 15), 30, 85),\n",
    "                'ra_initiated': np.random.randint(15, 25),\n",
    "                'ra_succeeded': lambda x: np.random.randint(max(1, int(x*0.4)), int(x*0.8)+1),\n",
    "                'failed_attempts': np.random.randint(5, 15)\n",
    "            }\n",
    "            base_signal = np.random.uniform(55, 75)  # è¼ƒé«˜çš„ä¿¡è™Ÿï¼ˆæ”»æ“Šè€…ä¿¡è™Ÿï¼‰\n",
    "            signal_variation = 10\n",
    "            \n",
    "        elif anomaly_type == 'interference':\n",
    "            # å¹²æ“¾å ´æ™¯ï¼šä¿¡è™Ÿä¸ç©©å®šã€æˆåŠŸç‡ä¸­ç­‰\n",
    "            ra_stats = {\n",
    "                'success_rate': np.clip(np.random.normal(70, 12), 45, 90),\n",
    "                'ra_initiated': np.random.randint(10, 20),\n",
    "                'ra_succeeded': lambda x: np.random.randint(max(1, int(x*0.5)), int(x*0.9)+1),\n",
    "                'failed_attempts': np.random.randint(2, 8)\n",
    "            }\n",
    "            base_signal = np.random.uniform(30, 50)  # è¼ƒä½çš„ä¿¡è™Ÿï¼ˆå¹²æ“¾å½±éŸ¿ï¼‰\n",
    "            signal_variation = 15\n",
    "            \n",
    "        else:  # hardware_failure\n",
    "            # ç¡¬é«”æ•…éšœå ´æ™¯ï¼šæ¥µä½æˆåŠŸç‡ã€æ¥µä¸ç©©å®š\n",
    "            ra_stats = {\n",
    "                'success_rate': np.clip(np.random.normal(40, 20), 10, 70),\n",
    "                'ra_initiated': np.random.randint(20, 30),\n",
    "                'ra_succeeded': lambda x: np.random.randint(1, max(2, int(x*0.5))),\n",
    "                'failed_attempts': np.random.randint(10, 20)\n",
    "            }\n",
    "            base_signal = np.random.uniform(20, 40)  # å¾ˆä½çš„ä¿¡è™Ÿï¼ˆç¡¬é«”å•é¡Œï¼‰\n",
    "            signal_variation = 20\n",
    "        \n",
    "        ra_stats['ra_succeeded'] = ra_stats['ra_succeeded'](ra_stats['ra_initiated'])\n",
    "        ra_stats['success_rate'] = (ra_stats['ra_succeeded'] / ra_stats['ra_initiated']) * 100\n",
    "        \n",
    "        # ç•°å¸¸æ™‚é–“æˆ³ï¼šä¸è¦å¾‹çš„é–“éš”\n",
    "        timestamps = []\n",
    "        current_time = 0\n",
    "        for _ in range(np.random.randint(5, 15)):\n",
    "            # æ›´å¤§çš„éš¨æ©Ÿè®Šå‹•ï¼Œæ¨¡æ“¬ä¸ç©©å®š\n",
    "            interval = np.random.exponential(2) + np.random.uniform(0.5, 4)\n",
    "            current_time += interval\n",
    "            timestamps.append(current_time)\n",
    "        \n",
    "        # ç•°å¸¸ä¿¡è™Ÿï¼šä¸ç©©å®šä¸”å¯èƒ½è¶…å‡ºæ­£å¸¸ç¯„åœ\n",
    "        signal_data = [base_signal + np.random.normal(0, signal_variation) for _ in range(10)]\n",
    "        \n",
    "        test_data = {\n",
    "            'ra_stats': ra_stats,\n",
    "            'test_duration': 20,\n",
    "            'attacker_active': True,  # ç•°å¸¸æƒ…æ³å‡è¨­æœ‰æ”»æ“Š\n",
    "            'airplane_toggles': np.random.randint(8, 15),\n",
    "            'loop_number': i,\n",
    "            'test_type': 'anomaly',\n",
    "            'timestamps': timestamps,\n",
    "            'signal_data': signal_data,\n",
    "            'anomaly_type': anomaly_type\n",
    "        }\n",
    "        anomaly_data.append(test_data)\n",
    "    \n",
    "    print(f\"âœ… åˆæˆæ•¸æ“šç”Ÿæˆå®Œæˆ\")\n",
    "    print(f\"   æ­£å¸¸æ¨£æœ¬: {len(normal_data)}\")\n",
    "    print(f\"   ç•°å¸¸æ¨£æœ¬: {len(anomaly_data)}\")\n",
    "    \n",
    "    return normal_data, anomaly_data\n",
    "\n",
    "# ç”Ÿæˆç¤ºä¾‹è¨“ç·´æ•¸æ“š\n",
    "normal_samples, anomaly_samples = generate_synthetic_training_data(100, 30)\n",
    "\n",
    "# åˆ†æç”Ÿæˆçš„æ•¸æ“š\n",
    "print(\"\\\\nğŸ“Š æ•¸æ“šåˆ†æ:\")\n",
    "\n",
    "# æ­£å¸¸æ•¸æ“šçµ±è¨ˆ\n",
    "normal_success_rates = [sample['ra_stats']['success_rate'] for sample in normal_samples]\n",
    "print(f\"æ­£å¸¸æ•¸æ“š - å¹³å‡æˆåŠŸç‡: {np.mean(normal_success_rates):.1f}% (Â±{np.std(normal_success_rates):.1f})\")\n",
    "\n",
    "# ç•°å¸¸æ•¸æ“šçµ±è¨ˆ\n",
    "anomaly_success_rates = [sample['ra_stats']['success_rate'] for sample in anomaly_samples]\n",
    "print(f\"ç•°å¸¸æ•¸æ“š - å¹³å‡æˆåŠŸç‡: {np.mean(anomaly_success_rates):.1f}% (Â±{np.std(anomaly_success_rates):.1f})\")\n",
    "\n",
    "# è¦–è¦ºåŒ–æ•¸æ“šåˆ†ä½ˆ\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(normal_success_rates, bins=20, alpha=0.7, label='æ­£å¸¸', color='green')\n",
    "plt.hist(anomaly_success_rates, bins=20, alpha=0.7, label='ç•°å¸¸', color='red')\n",
    "plt.xlabel('RAæˆåŠŸç‡ (%)')\n",
    "plt.ylabel('é »æ¬¡')\n",
    "plt.title('æˆåŠŸç‡åˆ†ä½ˆæ¯”è¼ƒ')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "normal_ra_counts = [sample['ra_stats']['ra_initiated'] for sample in normal_samples]\n",
    "anomaly_ra_counts = [sample['ra_stats']['ra_initiated'] for sample in anomaly_samples]\n",
    "plt.hist(normal_ra_counts, bins=15, alpha=0.7, label='æ­£å¸¸', color='green')\n",
    "plt.hist(anomaly_ra_counts, bins=15, alpha=0.7, label='ç•°å¸¸', color='red')\n",
    "plt.xlabel('RAå˜—è©¦æ¬¡æ•¸')\n",
    "plt.ylabel('é »æ¬¡')\n",
    "plt.title('RAå˜—è©¦æ¬¡æ•¸åˆ†ä½ˆæ¯”è¼ƒ')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31ca08",
   "metadata": {},
   "source": [
    "## 5. ç„¡ç›£ç£å­¸ç¿’æ¨¡å‹è¨“ç·´\n",
    "\n",
    "ç„¡ç›£ç£å­¸ç¿’ç®—æ³•åªéœ€è¦æ­£å¸¸æ•¸æ“šé€²è¡Œè¨“ç·´ï¼Œé©ç”¨æ–¼ç¼ºä¹æ¨™è¨˜ç•°å¸¸æ•¸æ“šçš„å ´æ™¯ã€‚æˆ‘å€‘å°‡å¯¦ä½œIsolation Forestã€One-Class SVMå’ŒDBSCANã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‚ºMLAnomalyDetectoré¡åˆ¥æ·»åŠ è¨“ç·´æ–¹æ³•\n",
    "def train_unsupervised(self, X: np.ndarray, feature_names: List[str] = None):\n",
    "    \"\"\"\n",
    "    ç„¡ç›£ç£å­¸ç¿’è¨“ç·´ï¼ˆåªä½¿ç”¨æ­£å¸¸æ•¸æ“šï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        X: è¨“ç·´ç‰¹å¾µçŸ©é™£ (n_samples, n_features)\n",
    "        feature_names: ç‰¹å¾µåç¨±åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if self.model_type in ['random_forest']:\n",
    "        print(f\"âš ï¸ {self.model_type} æ˜¯ç›£ç£å­¸ç¿’æ¨¡å‹ï¼Œéœ€è¦æ¨™ç±¤æ•¸æ“š\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸš€ é–‹å§‹è¨“ç·´ç„¡ç›£ç£æ¨¡å‹: {self.model_type}\")\n",
    "    print(f\"è¨“ç·´æ•¸æ“šå½¢ç‹€: {X.shape}\")\n",
    "    \n",
    "    # æ•¸æ“šæ¨™æº–åŒ–\n",
    "    X_scaled = self.scaler.fit_transform(X)\n",
    "    print(f\"âœ… æ•¸æ“šæ¨™æº–åŒ–å®Œæˆ\")\n",
    "    \n",
    "    # è¨“ç·´æ¨¡å‹\n",
    "    if self.model_type == 'dbscan':\n",
    "        # DBSCAN é€²è¡Œèšé¡ï¼Œç•°å¸¸é»æ¨™è¨˜ç‚º -1\n",
    "        labels = self.model.fit_predict(X_scaled)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_anomalies = sum(labels == -1)\n",
    "        print(f\"âœ… DBSCAN èšé¡å®Œæˆ\")\n",
    "        print(f\"   èšé¡æ•¸é‡: {n_clusters}\")\n",
    "        print(f\"   ç•°å¸¸é»æ•¸é‡: {n_anomalies} ({n_anomalies/len(labels)*100:.1f}%)\")\n",
    "    else:\n",
    "        # Isolation Forest å’Œ One-Class SVM\n",
    "        self.model.fit(X_scaled)\n",
    "        print(f\"âœ… {self.model_type} è¨“ç·´å®Œæˆ\")\n",
    "    \n",
    "    self.is_trained = True\n",
    "    self.feature_names = feature_names or []\n",
    "    \n",
    "    # åœ¨è¨“ç·´æ•¸æ“šä¸Šé€²è¡Œé æ¸¬ä»¥è©•ä¼°æ¨¡å‹\n",
    "    if self.model_type != 'dbscan':\n",
    "        predictions = self.model.predict(X_scaled)\n",
    "        n_anomalies = sum(predictions == -1)  # -1 è¡¨ç¤ºç•°å¸¸\n",
    "        print(f\"è¨“ç·´æ•¸æ“šä¸­æª¢æ¸¬åˆ°çš„ç•°å¸¸: {n_anomalies} ({n_anomalies/len(predictions)*100:.1f}%)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def predict_anomaly_batch(self, test_data_list: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    æ‰¹é‡é æ¸¬ç•°å¸¸\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        test_data_list: æ¸¬è©¦æ•¸æ“šåˆ—è¡¨\n",
    "        \n",
    "    è¿”å›:\n",
    "        é æ¸¬çµæœåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if not self.is_trained:\n",
    "        return [{'error': 'Model not trained'} for _ in test_data_list]\n",
    "    \n",
    "    results = []\n",
    "    extractor = MLFeatureExtractor()\n",
    "    \n",
    "    for test_data in test_data_list:\n",
    "        try:\n",
    "            # æå–ç‰¹å¾µ\n",
    "            feature_vector = extractor.extract_comprehensive_features(test_data)\n",
    "            \n",
    "            # ç¢ºä¿ç‰¹å¾µé †åºä¸€è‡´\n",
    "            if self.feature_names:\n",
    "                features = np.array([feature_vector.get(name, 0) for name in self.feature_names])\n",
    "            else:\n",
    "                sorted_keys = sorted(feature_vector.keys())\n",
    "                features = np.array([feature_vector[key] for key in sorted_keys])\n",
    "            \n",
    "            features = features.reshape(1, -1)\n",
    "            \n",
    "            # æ¨™æº–åŒ–\n",
    "            features_scaled = self.scaler.transform(features)\n",
    "            \n",
    "            # é æ¸¬\n",
    "            if self.model_type in ['isolation_forest', 'one_class_svm']:\n",
    "                prediction = self.model.predict(features_scaled)[0]\n",
    "                \n",
    "                if hasattr(self.model, 'decision_function'):\n",
    "                    decision_score = self.model.decision_function(features_scaled)[0]\n",
    "                    confidence = abs(decision_score)\n",
    "                    anomaly_score = -decision_score if self.model_type == 'isolation_forest' else decision_score\n",
    "                else:\n",
    "                    confidence = 0.5\n",
    "                    anomaly_score = 0\n",
    "                \n",
    "                result = {\n",
    "                    'is_anomaly': prediction == -1,\n",
    "                    'confidence': confidence,\n",
    "                    'anomaly_score': anomaly_score,\n",
    "                    'prediction_value': prediction\n",
    "                }\n",
    "                \n",
    "            elif self.model_type == 'dbscan':\n",
    "                result = {\n",
    "                    'is_anomaly': False,\n",
    "                    'confidence': 0,\n",
    "                    'anomaly_score': 0,\n",
    "                    'error': 'DBSCAN cannot predict single samples'\n",
    "                }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'is_anomaly': False,\n",
    "                'confidence': 0,\n",
    "                'anomaly_score': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# å°‡æ–¹æ³•æ·»åŠ åˆ°MLAnomalyDetectoré¡åˆ¥\n",
    "MLAnomalyDetector.train_unsupervised = train_unsupervised\n",
    "MLAnomalyDetector.predict_anomaly_batch = predict_anomaly_batch\n",
    "\n",
    "# è¨“ç·´ç„¡ç›£ç£æ¨¡å‹\n",
    "print(\"ğŸ”¬ é–‹å§‹è¨“ç·´ç„¡ç›£ç£å­¸ç¿’æ¨¡å‹...\")\n",
    "\n",
    "# æå–æ­£å¸¸æ•¸æ“šçš„ç‰¹å¾µ\n",
    "extractor = MLFeatureExtractor()\n",
    "X_normal = []\n",
    "feature_names = None\n",
    "\n",
    "for data in normal_samples:\n",
    "    features = extractor.extract_comprehensive_features(data)\n",
    "    if feature_names is None:\n",
    "        feature_names = sorted(features.keys())\n",
    "    feature_vector = np.array([features[name] for name in feature_names])\n",
    "    X_normal.append(feature_vector)\n",
    "\n",
    "X_normal = np.array(X_normal)\n",
    "print(f\"æ­£å¸¸æ•¸æ“šç‰¹å¾µçŸ©é™£å½¢ç‹€: {X_normal.shape}\")\n",
    "\n",
    "# è¨“ç·´ä¸åŒçš„ç„¡ç›£ç£æ¨¡å‹\n",
    "models = {\n",
    "    'isolation_forest': MLAnomalyDetector('isolation_forest'),\n",
    "    'one_class_svm': MLAnomalyDetector('one_class_svm'),\n",
    "    'dbscan': MLAnomalyDetector('dbscan')\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, detector in models.items():\n",
    "    print(f\"\\\\n{'='*50}\")\n",
    "    print(f\"è¨“ç·´æ¨¡å‹: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    success = detector.train_unsupervised(X_normal, feature_names)\n",
    "    if success:\n",
    "        trained_models[model_name] = detector\n",
    "        print(f\"âœ… {model_name} è¨“ç·´æˆåŠŸ\")\n",
    "    else:\n",
    "        print(f\"âŒ {model_name} è¨“ç·´å¤±æ•—\")\n",
    "\n",
    "print(f\"\\\\nâœ… ç„¡ç›£ç£å­¸ç¿’è¨“ç·´å®Œæˆï¼ŒæˆåŠŸè¨“ç·´ {len(trained_models)} å€‹æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5b769",
   "metadata": {},
   "source": [
    "## 6. ç›£ç£å­¸ç¿’æ¨¡å‹è¨“ç·´\n",
    "\n",
    "ç›£ç£å­¸ç¿’éœ€è¦æ¨™è¨˜çš„æ­£å¸¸å’Œç•°å¸¸æ•¸æ“šï¼Œå¯ä»¥æä¾›æ›´ç²¾ç¢ºçš„åˆ†é¡çµæœã€‚æˆ‘å€‘å°‡ä½¿ç”¨Random Foresté€²è¡Œç›£ç£å­¸ç¿’è¨“ç·´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‚ºMLAnomalyDetectoré¡åˆ¥æ·»åŠ ç›£ç£å­¸ç¿’è¨“ç·´æ–¹æ³•\n",
    "def train_supervised(self, X: np.ndarray, y: np.ndarray, feature_names: List[str] = None):\n",
    "    \"\"\"\n",
    "    ç›£ç£å­¸ç¿’è¨“ç·´ï¼ˆä½¿ç”¨æ¨™è¨˜çš„æ­£å¸¸å’Œç•°å¸¸æ•¸æ“šï¼‰\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        X: ç‰¹å¾µçŸ©é™£ (n_samples, n_features)\n",
    "        y: æ¨™ç±¤å‘é‡ (n_samples,) - 0è¡¨ç¤ºæ­£å¸¸ï¼Œ1è¡¨ç¤ºç•°å¸¸\n",
    "        feature_names: ç‰¹å¾µåç¨±åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if self.model_type not in ['random_forest']:\n",
    "        print(f\"âš ï¸ {self.model_type} ä¸æ˜¯ç›£ç£å­¸ç¿’æ¨¡å‹\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸš€ é–‹å§‹è¨“ç·´ç›£ç£å­¸ç¿’æ¨¡å‹: {self.model_type}\")\n",
    "    print(f\"è¨“ç·´æ•¸æ“šå½¢ç‹€: {X.shape}\")\n",
    "    print(f\"é¡åˆ¥åˆ†ä½ˆ - æ­£å¸¸: {sum(y==0)}, ç•°å¸¸: {sum(y==1)}\")\n",
    "    \n",
    "    # æ•¸æ“šæ¨™æº–åŒ–\n",
    "    X_scaled = self.scaler.fit_transform(X)\n",
    "    print(f\"âœ… æ•¸æ“šæ¨™æº–åŒ–å®Œæˆ\")\n",
    "    \n",
    "    # åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"æ•¸æ“šåˆ†å‰² - è¨“ç·´é›†: {X_train.shape[0]}, æ¸¬è©¦é›†: {X_test.shape[0]}\")\n",
    "    \n",
    "    # è¨“ç·´æ¨¡å‹\n",
    "    self.model.fit(X_train, y_train)\n",
    "    print(f\"âœ… {self.model_type} è¨“ç·´å®Œæˆ\")\n",
    "    \n",
    "    # è©•ä¼°æ¨¡å‹\n",
    "    y_pred = self.model.predict(X_test)\n",
    "    y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\\\nğŸ“Š æ¨¡å‹è©•ä¼°çµæœ:\")\n",
    "    print(\"åˆ†é¡å ±å‘Š:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['æ­£å¸¸', 'ç•°å¸¸']))\n",
    "    \n",
    "    # AUCåˆ†æ•¸\n",
    "    if len(np.unique(y)) == 2:\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"AUCåˆ†æ•¸: {auc_score:.3f}\")\n",
    "    \n",
    "    # æ··æ·†çŸ©é™£\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\\\næ··æ·†çŸ©é™£:\")\n",
    "    print(f\"     é æ¸¬\")\n",
    "    print(f\"å¯¦éš›  æ­£å¸¸  ç•°å¸¸\")\n",
    "    print(f\"æ­£å¸¸  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "    print(f\"ç•°å¸¸  {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "    \n",
    "    self.is_trained = True\n",
    "    self.feature_names = feature_names or []\n",
    "    \n",
    "    # ç‰¹å¾µé‡è¦æ€§åˆ†æ\n",
    "    if hasattr(self.model, 'feature_importances_') and feature_names:\n",
    "        self._analyze_feature_importance()\n",
    "    \n",
    "    return True, {\n",
    "        'auc_score': auc_score if len(np.unique(y)) == 2 else None,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "\n",
    "def _analyze_feature_importance(self):\n",
    "    \"\"\"åˆ†æç‰¹å¾µé‡è¦æ€§\"\"\"\n",
    "    if not hasattr(self.model, 'feature_importances_'):\n",
    "        return\n",
    "    \n",
    "    importances = self.model.feature_importances_\n",
    "    feature_importance = list(zip(self.feature_names, importances))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\\\nğŸ” å‰10å€‹æœ€é‡è¦çš„ç‰¹å¾µ:\")\n",
    "    for i, (feature, importance) in enumerate(feature_importance[:10]):\n",
    "        print(f\"  {i+1:2d}. {feature:<25}: {importance:.4f}\")\n",
    "    \n",
    "    # è¦–è¦ºåŒ–ç‰¹å¾µé‡è¦æ€§\n",
    "    top_features = feature_importance[:15]  # å–å‰15å€‹ç‰¹å¾µ\n",
    "    features, importances = zip(*top_features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(features)), importances)\n",
    "    plt.yticks(range(len(features)), features)\n",
    "    plt.xlabel('ç‰¹å¾µé‡è¦æ€§')\n",
    "    plt.title('Random Forest ç‰¹å¾µé‡è¦æ€§æ’å')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# å°‡æ–¹æ³•æ·»åŠ åˆ°MLAnomalyDetectoré¡åˆ¥\n",
    "MLAnomalyDetector.train_supervised = train_supervised\n",
    "MLAnomalyDetector._analyze_feature_importance = _analyze_feature_importance\n",
    "\n",
    "# æº–å‚™ç›£ç£å­¸ç¿’æ•¸æ“š\n",
    "print(\"ğŸ“š æº–å‚™ç›£ç£å­¸ç¿’æ•¸æ“š...\")\n",
    "\n",
    "# æå–æ‰€æœ‰æ•¸æ“šçš„ç‰¹å¾µï¼ˆæ­£å¸¸ + ç•°å¸¸ï¼‰\n",
    "all_data = normal_samples + anomaly_samples\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "for i, data in enumerate(all_data):\n",
    "    features = extractor.extract_comprehensive_features(data)\n",
    "    feature_vector = np.array([features[name] for name in feature_names])\n",
    "    X_all.append(feature_vector)\n",
    "    \n",
    "    # æ¨™ç±¤ï¼šæ­£å¸¸=0ï¼Œç•°å¸¸=1\n",
    "    if i < len(normal_samples):\n",
    "        y_all.append(0)  # æ­£å¸¸\n",
    "    else:\n",
    "        y_all.append(1)  # ç•°å¸¸\n",
    "\n",
    "X_all = np.array(X_all)\n",
    "y_all = np.array(y_all)\n",
    "\n",
    "print(f\"å®Œæ•´æ•¸æ“šé›†å½¢ç‹€: {X_all.shape}\")\n",
    "print(f\"æ¨™ç±¤åˆ†ä½ˆ - æ­£å¸¸: {sum(y_all==0)}, ç•°å¸¸: {sum(y_all==1)}\")\n",
    "\n",
    "# è¨“ç·´ç›£ç£å­¸ç¿’æ¨¡å‹\n",
    "print(f\"\\\\n{'='*50}\")\n",
    "print(f\"è¨“ç·´ç›£ç£å­¸ç¿’æ¨¡å‹: Random Forest\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "rf_detector = MLAnomalyDetector('random_forest')\n",
    "success, metrics = rf_detector.train_supervised(X_all, y_all, feature_names)\n",
    "\n",
    "if success:\n",
    "    trained_models['random_forest'] = rf_detector\n",
    "    print(f\"âœ… Random Forest è¨“ç·´æˆåŠŸ\")\n",
    "else:\n",
    "    print(f\"âŒ Random Forest è¨“ç·´å¤±æ•—\")\n",
    "\n",
    "print(f\"\\\\nâœ… ç›£ç£å­¸ç¿’è¨“ç·´å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf5680",
   "metadata": {},
   "source": [
    "## 7. æ¨¡å‹é©—è­‰èˆ‡è©•ä¼°\n",
    "\n",
    "è©•ä¼°è¨“ç·´å¥½çš„æ¨¡å‹æ€§èƒ½ï¼Œä½¿ç”¨æ¸¬è©¦æ•¸æ“šé€²è¡Œé æ¸¬ä¸¦åˆ†æå„æ¨¡å‹çš„æª¢æ¸¬èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ¸¬è©¦æ•¸æ“š\n",
    "print(\"ğŸ§ª ç”Ÿæˆæ¸¬è©¦æ•¸æ“š...\")\n",
    "test_normal, test_anomaly = generate_synthetic_training_data(20, 10)\n",
    "test_data = test_normal + test_anomaly\n",
    "test_labels = [0] * len(test_normal) + [1] * len(test_anomaly)\n",
    "\n",
    "print(f\"æ¸¬è©¦æ•¸æ“š - æ­£å¸¸: {len(test_normal)}, ç•°å¸¸: {len(test_anomaly)}\")\n",
    "\n",
    "# è©•ä¼°æ‰€æœ‰è¨“ç·´å¥½çš„æ¨¡å‹\n",
    "def evaluate_model(detector, test_data, test_labels, model_name):\n",
    "    \"\"\"è©•ä¼°å–®å€‹æ¨¡å‹çš„æ€§èƒ½\"\"\"\n",
    "    print(f\"\\\\nğŸ” è©•ä¼°æ¨¡å‹: {model_name}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if not detector.is_trained:\n",
    "        print(\"âŒ æ¨¡å‹æœªè¨“ç·´\")\n",
    "        return None\n",
    "    \n",
    "    # æ‰¹é‡é æ¸¬\n",
    "    predictions = detector.predict_anomaly_batch(test_data)\n",
    "    \n",
    "    # æå–é æ¸¬çµæœ\n",
    "    y_pred = []\n",
    "    confidences = []\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if 'error' in pred:\n",
    "            y_pred.append(0)  # é»˜èªç‚ºæ­£å¸¸\n",
    "            confidences.append(0)\n",
    "        else:\n",
    "            y_pred.append(1 if pred['is_anomaly'] else 0)\n",
    "            confidences.append(pred.get('confidence', 0))\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(test_labels)\n",
    "    \n",
    "    # è¨ˆç®—è©•ä¼°æŒ‡æ¨™\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # æ··æ·†çŸ©é™£\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    \n",
    "    # è¨ˆç®—å…¶ä»–æŒ‡æ¨™\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    print(f\"æº–ç¢ºç‡ (Accuracy):     {accuracy:.3f}\")\n",
    "    print(f\"ç²¾ç¢ºç‡ (Precision):    {precision:.3f}\")\n",
    "    print(f\"å¬å›ç‡ (Recall):       {recall:.3f}\")\n",
    "    print(f\"F1åˆ†æ•¸:                {f1:.3f}\")\n",
    "    print(f\"ç‰¹ç•°æ€§ (Specificity):  {specificity:.3f}\")\n",
    "    print(f\"å‡é™½æ€§ç‡ (FPR):        {false_positive_rate:.3f}\")\n",
    "    print(f\"å‡é™°æ€§ç‡ (FNR):        {false_negative_rate:.3f}\")\n",
    "    \n",
    "    print(f\"\\\\næ··æ·†çŸ©é™£:\")\n",
    "    print(f\"     é æ¸¬\")\n",
    "    print(f\"å¯¦éš›  æ­£å¸¸  ç•°å¸¸\")\n",
    "    print(f\"æ­£å¸¸  {tn:4d}  {fp:4d}\")\n",
    "    print(f\"ç•°å¸¸  {fn:4d}  {tp:4d}\")\n",
    "    \n",
    "    # è¿”å›è©•ä¼°çµæœ\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'specificity': specificity,\n",
    "        'fpr': false_positive_rate,\n",
    "        'fnr': false_negative_rate,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': y_pred,\n",
    "        'confidences': confidences\n",
    "    }\n",
    "\n",
    "# è©•ä¼°æ‰€æœ‰æ¨¡å‹\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, detector in trained_models.items():\n",
    "    result = evaluate_model(detector, test_data, test_labels, model_name)\n",
    "    if result:\n",
    "        evaluation_results[model_name] = result\n",
    "\n",
    "# æ¯”è¼ƒæ¨¡å‹æ€§èƒ½\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(f\"                    æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if evaluation_results:\n",
    "    # å‰µå»ºæ¯”è¼ƒè¡¨æ ¼\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity']\n",
    "    \n",
    "    print(f\"{'æ¨¡å‹':<20}\", end='')\n",
    "    for metric in metrics:\n",
    "        print(f\"{metric.upper():<12}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for model_name, result in evaluation_results.items():\n",
    "        print(f\"{model_name:<20}\", end='')\n",
    "        for metric in metrics:\n",
    "            print(f\"{result[metric]:<12.3f}\", end='')\n",
    "        print()\n",
    "    \n",
    "    # è¦–è¦ºåŒ–æ¯”è¼ƒ\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # æ€§èƒ½æŒ‡æ¨™æ¯”è¼ƒ\n",
    "    plt.subplot(2, 3, 1)\n",
    "    models = list(evaluation_results.keys())\n",
    "    accuracies = [evaluation_results[m]['accuracy'] for m in models]\n",
    "    plt.bar(models, accuracies, color='skyblue')\n",
    "    plt.title('æº–ç¢ºç‡æ¯”è¼ƒ')\n",
    "    plt.ylabel('æº–ç¢ºç‡')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    precisions = [evaluation_results[m]['precision'] for m in models]\n",
    "    plt.bar(models, precisions, color='lightgreen')\n",
    "    plt.title('ç²¾ç¢ºç‡æ¯”è¼ƒ')\n",
    "    plt.ylabel('ç²¾ç¢ºç‡')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    recalls = [evaluation_results[m]['recall'] for m in models]\n",
    "    plt.bar(models, recalls, color='salmon')\n",
    "    plt.title('å¬å›ç‡æ¯”è¼ƒ')\n",
    "    plt.ylabel('å¬å›ç‡')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    f1_scores = [evaluation_results[m]['f1_score'] for m in models]\n",
    "    plt.bar(models, f1_scores, color='gold')\n",
    "    plt.title('F1åˆ†æ•¸æ¯”è¼ƒ')\n",
    "    plt.ylabel('F1åˆ†æ•¸')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # ROCæ›²ç·šï¼ˆåƒ…é™ç›£ç£å­¸ç¿’æ¨¡å‹ï¼‰\n",
    "    plt.subplot(2, 3, 5)\n",
    "    for model_name, result in evaluation_results.items():\n",
    "        if model_name == 'random_forest':\n",
    "            # ç‚ºç›£ç£å­¸ç¿’æ¨¡å‹ç¹ªè£½ROCæ›²ç·š\n",
    "            detector = trained_models[model_name]\n",
    "            # é‡æ–°è¨ˆç®—æ¦‚ç‡é æ¸¬\n",
    "            X_test = []\n",
    "            for data in test_data:\n",
    "                features = extractor.extract_comprehensive_features(data)\n",
    "                feature_vector = np.array([features[name] for name in feature_names])\n",
    "                X_test.append(feature_vector)\n",
    "            \n",
    "            X_test = np.array(X_test)\n",
    "            X_test_scaled = detector.scaler.transform(X_test)\n",
    "            y_prob = detector.model.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(test_labels, y_prob)\n",
    "            auc = roc_auc_score(test_labels, y_prob)\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='éš¨æ©Ÿåˆ†é¡å™¨')\n",
    "    plt.xlabel('å‡é™½æ€§ç‡')\n",
    "    plt.ylabel('çœŸé™½æ€§ç‡')\n",
    "    plt.title('ROCæ›²ç·š')\n",
    "    plt.legend()\n",
    "    \n",
    "    # ä¿¡å¿ƒåº¦åˆ†ä½ˆ\n",
    "    plt.subplot(2, 3, 6)\n",
    "    for model_name, result in evaluation_results.items():\n",
    "        if model_name != 'dbscan':  # DBSCANæ²’æœ‰ä¿¡å¿ƒåº¦\n",
    "            confidences = result['confidences']\n",
    "            plt.hist(confidences, bins=20, alpha=0.5, label=model_name)\n",
    "    \n",
    "    plt.xlabel('é æ¸¬ä¿¡å¿ƒåº¦')\n",
    "    plt.ylabel('é »æ¬¡')\n",
    "    plt.title('é æ¸¬ä¿¡å¿ƒåº¦åˆ†ä½ˆ')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ¨è–¦æœ€ä½³æ¨¡å‹\n",
    "    print(f\"\\\\nğŸ† æ¨¡å‹æ¨è–¦:\")\n",
    "    best_f1_model = max(evaluation_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "    best_accuracy_model = max(evaluation_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    \n",
    "    print(f\"æœ€ä½³F1åˆ†æ•¸: {best_f1_model[0]} (F1 = {best_f1_model[1]['f1_score']:.3f})\")\n",
    "    print(f\"æœ€ä½³æº–ç¢ºç‡: {best_accuracy_model[0]} (Accuracy = {best_accuracy_model[1]['accuracy']:.3f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰å¯è©•ä¼°çš„æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db070a",
   "metadata": {},
   "source": [
    "## 8. å¯¦æ™‚ç•°å¸¸åµæ¸¬å¯¦ä½œ\n",
    "\n",
    "å»ºç«‹RealTimeAnomalyDetectoré¡åˆ¥ï¼Œå¯¦ç¾é€£çºŒç›£æ§ã€é–¾å€¼è¨­å®šã€è­¦å ±æ©Ÿåˆ¶ç­‰å¯¦æ™‚åµæ¸¬åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeAnomalyDetector:\n",
    "    \"\"\"\n",
    "    å¯¦æ™‚ç•°å¸¸åµæ¸¬ç³»çµ± - æ•´åˆMLæ¨¡å‹åˆ°ç¾æœ‰çš„æ¸¬è©¦æ¡†æ¶\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, detector: MLAnomalyDetector = None, alert_callback: Callable = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å¯¦æ™‚ç•°å¸¸åµæ¸¬å™¨\n",
    "        \n",
    "        åƒæ•¸:\n",
    "            detector: è¨“ç·´å¥½çš„MLAnomalyDetector\n",
    "            alert_callback: ç•°å¸¸è­¦å ±å›èª¿å‡½æ•¸\n",
    "        \"\"\"\n",
    "        self.detector = detector\n",
    "        self.feature_extractor = MLFeatureExtractor()\n",
    "        self.alert_callback = alert_callback\n",
    "        self.is_monitoring = False\n",
    "        self.data_queue = queue.Queue()\n",
    "        self.monitoring_thread = None\n",
    "        \n",
    "        # ç•°å¸¸åµæ¸¬åƒæ•¸\n",
    "        self.alert_threshold = 0.7\n",
    "        self.consecutive_anomalies_threshold = 3\n",
    "        self.time_window_minutes = 5\n",
    "        \n",
    "        # çµ±è¨ˆè³‡æ–™\n",
    "        self.anomaly_history = []\n",
    "        self.total_predictions = 0\n",
    "        self.total_anomalies = 0\n",
    "        self.consecutive_anomalies = 0\n",
    "        \n",
    "        print(f\"âœ… RealTimeAnomalyDetector åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def set_detector(self, detector: MLAnomalyDetector):\n",
    "        \"\"\"è¨­ç½®MLåµæ¸¬å™¨\"\"\"\n",
    "        self.detector = detector\n",
    "        print(f\"âœ… MLåµæ¸¬å™¨å·²è¨­ç½®: {detector.model_type}\")\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"é–‹å§‹å¯¦æ™‚ç›£æ§\"\"\"\n",
    "        if self.is_monitoring:\n",
    "            print(\"âš ï¸ ç›£æ§å·²ç¶“åœ¨é‹è¡Œä¸­\")\n",
    "            return False\n",
    "        \n",
    "        if not self.detector or not self.detector.is_trained:\n",
    "            print(\"âŒ æ²’æœ‰å¯ç”¨çš„è¨“ç·´æ¨¡å‹\")\n",
    "            return False\n",
    "        \n",
    "        self.is_monitoring = True\n",
    "        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)\n",
    "        self.monitoring_thread.daemon = True\n",
    "        self.monitoring_thread.start()\n",
    "        \n",
    "        print(f\"ğŸš€ å¯¦æ™‚ç•°å¸¸ç›£æ§å·²å•Ÿå‹• (æ¨¡å‹: {self.detector.model_type})\")\n",
    "        return True\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"åœæ­¢å¯¦æ™‚ç›£æ§\"\"\"\n",
    "        self.is_monitoring = False\n",
    "        if self.monitoring_thread:\n",
    "            self.monitoring_thread.join(timeout=5)\n",
    "        \n",
    "        print(f\"â¹ï¸ å¯¦æ™‚ç•°å¸¸ç›£æ§å·²åœæ­¢\")\n",
    "    \n",
    "    def feed_data(self, test_data: Dict):\n",
    "        \"\"\"é¤µå…¥æ–°çš„æ¸¬è©¦æ•¸æ“š\"\"\"\n",
    "        if not self.is_monitoring:\n",
    "            return\n",
    "        \n",
    "        # æ·»åŠ æ™‚é–“æˆ³\n",
    "        test_data['timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        try:\n",
    "            self.data_queue.put(test_data, timeout=1)\n",
    "        except queue.Full:\n",
    "            print(\"âš ï¸ æ•¸æ“šéšŠåˆ—å·²æ»¿ï¼Œæ¸…ç†èˆŠæ•¸æ“š\")\n",
    "            # æ¸…ç©ºä¸€åŠçš„èˆŠæ•¸æ“š\n",
    "            for _ in range(self.data_queue.qsize() // 2):\n",
    "                try:\n",
    "                    self.data_queue.get_nowait()\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "            self.data_queue.put(test_data)\n",
    "    \n",
    "    def _monitoring_loop(self):\n",
    "        \"\"\"ç›£æ§ä¸»å¾ªç’°\"\"\"\n",
    "        print(f\"ğŸ”„ ç›£æ§ä¸»å¾ªç’°å·²å•Ÿå‹•\")\n",
    "        \n",
    "        while self.is_monitoring:\n",
    "            try:\n",
    "                # å¾éšŠåˆ—ç²å–æ•¸æ“š\n",
    "                test_data = self.data_queue.get(timeout=1)\n",
    "                \n",
    "                # é€²è¡Œç•°å¸¸åµæ¸¬\n",
    "                predictions = self.detector.predict_anomaly_batch([test_data])\n",
    "                \n",
    "                if predictions and not predictions[0].get('error'):\n",
    "                    result = predictions[0]\n",
    "                    \n",
    "                    # æ›´æ–°çµ±è¨ˆ\n",
    "                    self.total_predictions += 1\n",
    "                    \n",
    "                    if result.get('is_anomaly', False):\n",
    "                        self.total_anomalies += 1\n",
    "                        self.consecutive_anomalies += 1\n",
    "                        \n",
    "                        # è¨˜éŒ„ç•°å¸¸\n",
    "                        anomaly_record = {\\n                            'timestamp': test_data.get('timestamp'),\\n                            'confidence': result.get('confidence', 0),\\n                            'anomaly_score': result.get('anomaly_score', 0),\\n                            'test_data': test_data,\\n                            'result': result,\\n                            'consecutive_count': self.consecutive_anomalies\\n                        }\\n                        self.anomaly_history.append(anomaly_record)\\n                        \\n                        # æª¢æŸ¥æ˜¯å¦éœ€è¦ç™¼å‡ºè­¦å ±\\n                        if (result.get('confidence', 0) >= self.alert_threshold or \\n                            self.consecutive_anomalies >= self.consecutive_anomalies_threshold):\\n                            self._trigger_alert(anomaly_record)\\n                        \\n                        print(f\\\"ğŸš¨ ç•°å¸¸åµæ¸¬ - ä¿¡å¿ƒåº¦: {result.get('confidence', 0):.3f}, é€£çºŒæ¬¡æ•¸: {self.consecutive_anomalies}\\\")\\n                        \\n                    else:\\n                        self.consecutive_anomalies = 0  # é‡ç½®é€£çºŒç•°å¸¸è¨ˆæ•¸\\n                \\n                # æ¸…ç†èˆŠçš„ç•°å¸¸è¨˜éŒ„\\n                self._cleanup_old_records()\\n                \\n            except queue.Empty:\\n                continue\\n            except Exception as e:\\n                print(f\\\"âŒ ç›£æ§å¾ªç’°éŒ¯èª¤: {e}\\\")\\n    \\n    def _trigger_alert(self, anomaly_record: Dict):\\n        \\\"\\\"\\\"è§¸ç™¼ç•°å¸¸è­¦å ±\\\"\\\"\\\"\\n        alert_message = f\\\"âš ï¸ é«˜ä¿¡å¿ƒåº¦ç•°å¸¸åµæ¸¬è­¦å ±!\\\"\\n        alert_details = {\\n            'timestamp': anomaly_record['timestamp'],\\n            'confidence': anomaly_record['confidence'],\\n            'anomaly_score': anomaly_record['anomaly_score'],\\n            'consecutive_count': anomaly_record['consecutive_count'],\\n            'model_type': self.detector.model_type\\n        }\\n        \\n        print(f\\\"ğŸš¨ {alert_message}\\\")\\n        print(f\\\"ğŸš¨ è©³ç´°è³‡è¨Š: {alert_details}\\\")\\n        \\n        # å‘¼å«è‡ªå®šç¾©è­¦å ±å›èª¿\\n        if self.alert_callback:\\n            try:\\n                self.alert_callback(alert_message, alert_details, anomaly_record)\\n            except Exception as e:\\n                print(f\\\"âŒ è­¦å ±å›èª¿å¤±æ•—: {e}\\\")\\n    \\n    def _cleanup_old_records(self):\\n        \\\"\\\"\\\"æ¸…ç†è¶…éæ™‚é–“çª—å£çš„èˆŠè¨˜éŒ„\\\"\\\"\\\"\\n        cutoff_time = datetime.now() - timedelta(minutes=self.time_window_minutes)\\n        \\n        self.anomaly_history = [\\n            record for record in self.anomaly_history\\n            if datetime.fromisoformat(record['timestamp']) > cutoff_time\\n        ]\\n    \\n    def get_statistics(self) -> Dict:\\n        \\\"\\\"\\\"ç²å–å¯¦æ™‚çµ±è¨ˆè³‡æ–™\\\"\\\"\\\"\\n        recent_anomalies = len([\\n            r for r in self.anomaly_history\\n            if datetime.fromisoformat(r['timestamp']) > datetime.now() - timedelta(minutes=self.time_window_minutes)\\n        ])\\n        \\n        anomaly_rate = (self.total_anomalies / max(self.total_predictions, 1)) * 100\\n        \\n        return {\\n            'total_predictions': self.total_predictions,\\n            'total_anomalies': self.total_anomalies,\\n            'anomaly_rate': anomaly_rate,\\n            'recent_anomalies': recent_anomalies,\\n            'consecutive_anomalies': self.consecutive_anomalies,\\n            'is_monitoring': self.is_monitoring,\\n            'queue_size': self.data_queue.qsize(),\\n            'model_type': self.detector.model_type if self.detector else None\\n        }\\n    \\n    def set_alert_threshold(self, threshold: float):\\n        \\\"\\\"\\\"è¨­å®šè­¦å ±é–¾å€¼\\\"\\\"\\\"\\n        self.alert_threshold = threshold\\n        print(f\\\"ğŸ”§ è­¦å ±é–¾å€¼å·²è¨­å®šç‚º {threshold}\\\")\\n    \\n    def set_consecutive_threshold(self, count: int):\\n        \\\"\\\"\\\"è¨­å®šé€£çºŒç•°å¸¸è­¦å ±é–¾å€¼\\\"\\\"\\\"\\n        self.consecutive_anomalies_threshold = count\\n        print(f\\\"ğŸ”§ é€£çºŒç•°å¸¸é–¾å€¼å·²è¨­å®šç‚º {count}\\\")\\n\\n# æ¸¬è©¦å¯¦æ™‚ç•°å¸¸åµæ¸¬ç³»çµ±\\nprint(\\\"\\\\nğŸ§ª æ¸¬è©¦å¯¦æ™‚ç•°å¸¸åµæ¸¬ç³»çµ±...\\\")\\n\\n# å®šç¾©è­¦å ±å›èª¿å‡½æ•¸\\ndef alert_callback(message: str, details: Dict, record: Dict):\\n    print(f\\\"ğŸ“¢ è­¦å ±å›èª¿: {message}\\\")\\n    print(f\\\"ğŸ“¢ æ¨¡å‹é¡å‹: {details['model_type']}\\\")\\n    print(f\\\"ğŸ“¢ ç•°å¸¸åˆ†æ•¸: {details['anomaly_score']:.3f}\\\")\\n\\n# é¸æ“‡æœ€ä½³æ¨¡å‹é€²è¡Œå¯¦æ™‚åµæ¸¬\\nif evaluation_results:\\n    best_model_name = max(evaluation_results.items(), key=lambda x: x[1]['f1_score'])[0]\\n    best_detector = trained_models[best_model_name]\\n    \\n    print(f\\\"é¸æ“‡æœ€ä½³æ¨¡å‹é€²è¡Œå¯¦æ™‚åµæ¸¬: {best_model_name}\\\")\\n    \\n    # å‰µå»ºå¯¦æ™‚åµæ¸¬å™¨\\n    rt_detector = RealTimeAnomalyDetector(best_detector, alert_callback)\\n    rt_detector.set_alert_threshold(0.6)  # è¼ƒä½çš„é–¾å€¼ä»¥å¢åŠ æ•æ„Ÿåº¦\\n    rt_detector.set_consecutive_threshold(2)\\n    \\n    # å•Ÿå‹•ç›£æ§\\n    rt_detector.start_monitoring()\\n    \\n    # æ¨¡æ“¬é¤µå…¥æ¸¬è©¦æ•¸æ“š\\n    print(\\\"\\\\nğŸ”„ æ¨¡æ“¬å¯¦æ™‚æ•¸æ“šæµ...\\\")\\n    \\n    # é¤µå…¥ä¸€äº›æ­£å¸¸æ•¸æ“š\\n    for i in range(5):\\n        normal_data = test_normal[i % len(test_normal)]\\n        rt_detector.feed_data(normal_data)\\n        time.sleep(0.1)\\n    \\n    # é¤µå…¥ä¸€äº›ç•°å¸¸æ•¸æ“š\\n    for i in range(3):\\n        anomaly_data = test_anomaly[i % len(test_anomaly)]\\n        rt_detector.feed_data(anomaly_data)\\n        time.sleep(0.1)\\n    \\n    # ç­‰å¾…è™•ç†å®Œæˆ\\n    time.sleep(2)\\n    \\n    # ç²å–çµ±è¨ˆè³‡æ–™\\n    stats = rt_detector.get_statistics()\\n    print(f\\\"\\\\nğŸ“Š å¯¦æ™‚åµæ¸¬çµ±è¨ˆ:\\\")\\n    for key, value in stats.items():\\n        print(f\\\"  {key}: {value}\\\")\\n    \\n    # åœæ­¢ç›£æ§\\n    rt_detector.stop_monitoring()\\n    \\nelse:\\n    print(\\\"âŒ æ²’æœ‰å¯ç”¨çš„è¨“ç·´æ¨¡å‹é€²è¡Œå¯¦æ™‚åµæ¸¬æ¸¬è©¦\\\")\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836a2ea",
   "metadata": {},
   "source": [
    "## 9. æ¨¡å‹ä¿å­˜èˆ‡è¼‰å…¥\n",
    "\n",
    "å¯¦ä½œæ¨¡å‹åºåˆ—åŒ–åŠŸèƒ½ï¼Œå…è¨±ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹ä¸¦åœ¨å¾ŒçºŒæ¸¬è©¦ä¸­é‡è¤‡ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ab5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‚ºMLAnomalyDetectoré¡åˆ¥æ·»åŠ ä¿å­˜å’Œè¼‰å…¥æ–¹æ³•\\ndef save_model(self, filepath: str):\\n    \\\"\\\"\\\"å„²å­˜è¨“ç·´å¥½çš„æ¨¡å‹\\\"\\\"\\\"\\n    if not self.is_trained:\\n        print(\\\"âš ï¸ æ²’æœ‰å·²è¨“ç·´çš„æ¨¡å‹å¯ä»¥ä¿å­˜\\\")\\n        return False\\n    \\n    model_data = {\\n        'model': self.model,\\n        'scaler': self.scaler,\\n        'model_type': self.model_type,\\n        'feature_names': self.feature_names,\\n        'threshold': self.threshold,\\n        'timestamp': datetime.now().isoformat(),\\n        'is_trained': self.is_trained\\n    }\\n    \\n    try:\\n        joblib.dump(model_data, filepath)\\n        print(f\\\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}\\\")\\n        return True\\n    except Exception as e:\\n        print(f\\\"âŒ æ¨¡å‹ä¿å­˜å¤±æ•—: {e}\\\")\\n        return False\\n\\ndef load_model(self, filepath: str):\\n    \\\"\\\"\\\"è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹\\\"\\\"\\\"\\n    try:\\n        model_data = joblib.load(filepath)\\n        \\n        self.model = model_data['model']\\n        self.scaler = model_data['scaler']\\n        self.model_type = model_data['model_type']\\n        self.feature_names = model_data.get('feature_names', [])\\n        self.threshold = model_data.get('threshold', 0.5)\\n        self.is_trained = model_data.get('is_trained', True)\\n        \\n        print(f\\\"âœ… æ¨¡å‹å·²å¾ {filepath} è¼‰å…¥\\\")\\n        print(f\\\"   æ¨¡å‹é¡å‹: {self.model_type}\\\")\\n        print(f\\\"   ç‰¹å¾µæ•¸é‡: {len(self.feature_names)}\\\")\\n        print(f\\\"   è¨“ç·´æ™‚é–“: {model_data.get('timestamp', 'Unknown')}\\\")\\n        return True\\n        \\n    except Exception as e:\\n        print(f\\\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\\\")\\n        return False\\n\\n# å°‡æ–¹æ³•æ·»åŠ åˆ°MLAnomalyDetectoré¡åˆ¥\\nMLAnomalyDetector.save_model = save_model\\nMLAnomalyDetector.load_model = load_model\\n\\n# ä¿å­˜æ‰€æœ‰è¨“ç·´å¥½çš„æ¨¡å‹\\nprint(\\\"ğŸ’¾ ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹...\\\")\\n\\nmodel_save_dir = \\\"/home/ksmo/ml_models\\\"\\nimport os\\nos.makedirs(model_save_dir, exist_ok=True)\\n\\nsaved_models = {}\\n\\nfor model_name, detector in trained_models.items():\\n    filepath = os.path.join(model_save_dir, f\\\"5g_anomaly_{model_name}.joblib\\\")\\n    success = detector.save_model(filepath)\\n    if success:\\n        saved_models[model_name] = filepath\\n\\nprint(f\\\"\\\\nâœ… æˆåŠŸä¿å­˜ {len(saved_models)} å€‹æ¨¡å‹:\\\")\\nfor model_name, filepath in saved_models.items():\\n    print(f\\\"  {model_name}: {filepath}\\\")\\n\\n# æ¸¬è©¦æ¨¡å‹è¼‰å…¥\\nprint(f\\\"\\\\nğŸ”„ æ¸¬è©¦æ¨¡å‹è¼‰å…¥åŠŸèƒ½...\\\")\\n\\nif saved_models:\\n    # é¸æ“‡ä¸€å€‹æ¨¡å‹é€²è¡Œè¼‰å…¥æ¸¬è©¦\\n    test_model_name = list(saved_models.keys())[0]\\n    test_filepath = saved_models[test_model_name]\\n    \\n    # å‰µå»ºæ–°çš„åµæ¸¬å™¨ä¸¦è¼‰å…¥æ¨¡å‹\\n    new_detector = MLAnomalyDetector(test_model_name)\\n    success = new_detector.load_model(test_filepath)\\n    \\n    if success:\\n        print(f\\\"âœ… æ¨¡å‹è¼‰å…¥æ¸¬è©¦æˆåŠŸ\\\")\\n        \\n        # é©—è­‰è¼‰å…¥çš„æ¨¡å‹åŠŸèƒ½\\n        test_sample = test_data[0]\\n        predictions = new_detector.predict_anomaly_batch([test_sample])\\n        \\n        if predictions and not predictions[0].get('error'):\\n            result = predictions[0]\\n            print(f\\\"   é æ¸¬æ¸¬è©¦: ç•°å¸¸={result['is_anomaly']}, ä¿¡å¿ƒåº¦={result.get('confidence', 0):.3f}\\\")\\n        else:\\n            print(f\\\"   âš ï¸ é æ¸¬æ¸¬è©¦å¤±æ•—\\\")\\n    else:\\n        print(f\\\"âŒ æ¨¡å‹è¼‰å…¥æ¸¬è©¦å¤±æ•—\\\")\\n\\n# å‰µå»ºæ¨¡å‹è³‡è¨Šæ‘˜è¦\\nmodel_summary = {\\n    'training_date': datetime.now().isoformat(),\\n    'models_trained': list(trained_models.keys()),\\n    'evaluation_results': evaluation_results,\\n    'best_model': {\\n        'name': max(evaluation_results.items(), key=lambda x: x[1]['f1_score'])[0] if evaluation_results else None,\\n        'f1_score': max(evaluation_results.items(), key=lambda x: x[1]['f1_score'])[1]['f1_score'] if evaluation_results else None\\n    },\\n    'training_data_summary': {\\n        'normal_samples': len(normal_samples),\\n        'anomaly_samples': len(anomaly_samples),\\n        'features_count': len(feature_names),\\n        'feature_names': feature_names\\n    }\\n}\\n\\n# ä¿å­˜è¨“ç·´æ‘˜è¦\\nsummary_filepath = os.path.join(model_save_dir, \\\"training_summary.json\\\")\\nwith open(summary_filepath, 'w', encoding='utf-8') as f:\\n    json.dump(model_summary, f, indent=2, ensure_ascii=False)\\n\\nprint(f\\\"\\\\nğŸ“‹ è¨“ç·´æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_filepath}\\\")\\n\\n# é¡¯ç¤ºä¿å­˜çš„æª”æ¡ˆ\\nprint(f\\\"\\\\nğŸ“ ä¿å­˜çš„æª”æ¡ˆåˆ—è¡¨:\\\")\\nfor filename in os.listdir(model_save_dir):\\n    filepath = os.path.join(model_save_dir, filename)\\n    file_size = os.path.getsize(filepath) / 1024  # KB\\n    print(f\\\"  {filename:<35} ({file_size:.1f} KB)\\\")\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c25bc4",
   "metadata": {},
   "source": [
    "## 10. æ•´åˆåˆ°æ¸¬è©¦ç³»çµ±ä¸­\n",
    "\n",
    "å°‡MLç•°å¸¸åµæ¸¬åŠŸèƒ½æ•´åˆåˆ°TestSelectorç³»çµ±ä¸­ï¼ŒåŒ…å«é…ç½®ä»‹é¢ã€çµ±è¨ˆå ±å‘Šå’Œè­¦å ±è™•ç†æ©Ÿåˆ¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºå¦‚ä½•åœ¨TestSelectorä¸­ä½¿ç”¨MLç•°å¸¸åµæ¸¬\\n\\nclass TestSelectorMLIntegration:\\n    \\\"\\\"\\\"\\n    æ¼”ç¤ºTestSelectorèˆ‡MLç•°å¸¸åµæ¸¬çš„æ•´åˆ\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        self.ml_enabled = False\\n        self.ml_detector = None\\n        self.real_time_detector = None\\n        \\n    def enable_ml_detection(self, model_path: str = None, model_type: str = 'isolation_forest'):\\n        \\\"\\\"\\\"å•Ÿç”¨MLç•°å¸¸åµæ¸¬\\\"\\\"\\\"\\n        try:\\n            self.ml_detector = MLAnomalyDetector(model_type=model_type)\\n            \\n            if model_path and os.path.exists(model_path):\\n                success = self.ml_detector.load_model(model_path)\\n                if success:\\n                    print(f\\\"âœ… å·²è¼‰å…¥é è¨“ç·´æ¨¡å‹: {model_path}\\\")\\n                else:\\n                    print(f\\\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå°‡ä½¿ç”¨æœªè¨“ç·´çš„æ¨¡å‹\\\")\\n            else:\\n                print(f\\\"âš ï¸ æœªæŒ‡å®šæ¨¡å‹è·¯å¾‘æˆ–æª”æ¡ˆä¸å­˜åœ¨ï¼Œæ¨¡å‹éœ€è¦è¨“ç·´\\\")\\n            \\n            # åˆå§‹åŒ–å¯¦æ™‚åµæ¸¬å™¨\\n            self.real_time_detector = RealTimeAnomalyDetector(\\n                detector=self.ml_detector,\\n                alert_callback=self._handle_ml_alert\\n            )\\n            \\n            self.ml_enabled = True\\n            print(f\\\"âœ… MLç•°å¸¸åµæ¸¬å·²å•Ÿç”¨ (æ¨¡å‹: {model_type})\\\")\\n            \\n        except Exception as e:\\n            print(f\\\"âŒ MLåµæ¸¬å•Ÿç”¨å¤±æ•—: {e}\\\")\\n            self.ml_enabled = False\\n    \\n    def _handle_ml_alert(self, message: str, details: Dict, record: Dict):\\n        \\\"\\\"\\\"è™•ç†MLç•°å¸¸è­¦å ±\\\"\\\"\\\"\\n        print(f\\\"ğŸš¨ MLç•°å¸¸è­¦å ±: {message}\\\")\\n        print(f\\\"ğŸš¨ ä¿¡å¿ƒåº¦: {details['confidence']:.3f}\\\")\\n        print(f\\\"ğŸš¨ ç•°å¸¸åˆ†æ•¸: {details['anomaly_score']:.3f}\\\")\\n        print(f\\\"ğŸš¨ é€£çºŒç•°å¸¸æ¬¡æ•¸: {details['consecutive_count']}\\\")\\n        \\n        # é€™è£¡å¯ä»¥æ·»åŠ è‡ªå‹•å›æ‡‰é‚è¼¯:\\n        # 1. è¨˜éŒ„åˆ°å°ˆç”¨æ—¥èªŒæª”æ¡ˆ\\n        # 2. ç™¼é€ç·Šæ€¥é€šçŸ¥çµ¦ç®¡ç†å“¡\\n        # 3. è§¸ç™¼é¡å¤–çš„è¨ºæ–·ç¨‹åº\\n        # 4. åœ¨åš´é‡æƒ…æ³ä¸‹è‡ªå‹•åœæ­¢æ¸¬è©¦\\n    \\n    def enhanced_analyze_ra_procedure(self, ra_stats: Dict, loop_number: int, test_type: str = \\\"standard\\\") -> Dict:\\n        \\\"\\\"\\\"å¢å¼·çš„RAç¨‹åºåˆ†æï¼ˆæ•´åˆMLåµæ¸¬ï¼‰\\\"\\\"\\\"\\n        \\n        # åŸå§‹RAåˆ†æ\\n        enhanced_stats = ra_stats.copy()\\n        \\n        # å¦‚æœMLåµæ¸¬å·²å•Ÿç”¨ä¸”æ¨¡å‹å·²è¨“ç·´\\n        if self.ml_enabled and self.ml_detector and self.ml_detector.is_trained:\\n            print(f\\\"ğŸ” åŸ·è¡ŒMLåˆ†æ - å¾ªç’° {loop_number} ({test_type})\\\")\\n            \\n            # æ ¹æ“šæ¸¬è©¦é¡å‹èª¿æ•´é æœŸè¡Œç‚º\\n            if test_type == \\\"cots_only\\\":\\n                base_signal = 45\\n                signal_variation = 3\\n                expected_toggles = 3\\n            else:\\n                base_signal = 55\\n                signal_variation = 8\\n                expected_toggles = 5\\n            \\n            # æº–å‚™MLè¼¸å…¥æ•¸æ“š\\n            current_time = time.time()\\n            test_data = {\\n                'ra_stats': ra_stats,\\n                'test_duration': 20,\\n                'attacker_active': test_type != \\\"cots_only\\\",\\n                'airplane_toggles': expected_toggles,\\n                'loop_number': loop_number,\\n                'test_type': test_type,\\n                'timestamps': [current_time - (20 - i*2) for i in range(min(ra_stats.get('ra_initiated', 5), 10))],\\n                'signal_data': [base_signal + (i * signal_variation / 10) + \\n                              np.random.normal(0, 2 if test_type == \\\"cots_only\\\" else 5) \\n                              for i in range(10)]\\n            }\\n            \\n            # åŸ·è¡ŒMLç•°å¸¸åµæ¸¬\\n            try:\\n                predictions = self.ml_detector.predict_anomaly_batch([test_data])\\n                \\n                if predictions and not predictions[0].get('error'):\\n                    ml_result = predictions[0]\\n                    \\n                    # é‡å°COTS onlyæ¨¡å¼èª¿æ•´åˆ¤æ–·é‚è¼¯\\n                    if test_type == \\\"cots_only\\\":\\n                        adjusted_threshold = 0.8\\n                        is_anomaly_adjusted = (ml_result.get('is_anomaly', False) and \\n                                              ml_result.get('confidence', 0) > adjusted_threshold)\\n                        \\n                        if ml_result.get('is_anomaly', False) and not is_anomaly_adjusted:\\n                            ml_result['is_anomaly'] = False\\n                            ml_result['confidence'] = ml_result.get('confidence', 0) * 0.5\\n                    \\n                    # å°‡MLçµæœæ•´åˆåˆ°RAçµ±è¨ˆä¸­\\n                    enhanced_stats.update({\\n                        'ml_anomaly_detected': ml_result.get('is_anomaly', False),\\n                        'ml_confidence': ml_result.get('confidence', 0),\\n                        'ml_anomaly_score': ml_result.get('anomaly_score', 0),\\n                        'ml_model_type': self.ml_detector.model_type,\\n                        'ml_test_type': test_type\\n                    })\\n                    \\n                    # é¤µå…¥å¯¦æ™‚åµæ¸¬å™¨\\n                    if self.real_time_detector and self.real_time_detector.is_monitoring:\\n                        self.real_time_detector.feed_data(test_data)\\n                    \\n                    # é¡¯ç¤ºMLåˆ†æçµæœ\\n                    if ml_result.get('is_anomaly', False):\\n                        print(f\\\"ğŸ” MLåˆ†æ: å¾ªç’° {loop_number} æª¢æ¸¬åˆ°ç•°å¸¸ ({test_type})!\\\")\\n                        print(f\\\"ğŸ” ä¿¡å¿ƒåº¦: {ml_result.get('confidence', 0):.3f}\\\")\\n                        if test_type == \\\"cots_only\\\":\\n                            print(f\\\"âš ï¸ æ³¨æ„: COTS-onlyæ¨¡å¼ä¸‹æª¢æ¸¬åˆ°ç•°å¸¸ï¼ˆä¸å°‹å¸¸ï¼‰\\\")\\n                    else:\\n                        print(f\\\"âœ… MLåˆ†æ: å¾ªç’° {loop_number} è¡Œç‚ºæ­£å¸¸ ({test_type})\\\")\\n                        \\n                else:\\n                    print(f\\\"âŒ MLåˆ†æå¤±æ•—: {predictions[0].get('error', 'Unknown error')}\\\")\\n                    \\n            except Exception as e:\\n                print(f\\\"âŒ MLåˆ†æç•°å¸¸: {e}\\\")\\n                enhanced_stats.update({\\n                    'ml_anomaly_detected': False,\\n                    'ml_confidence': 0,\\n                    'ml_error': str(e)\\n                })\\n        else:\\n            if self.ml_enabled and self.ml_detector and not self.ml_detector.is_trained:\\n                print(f\\\"â„¹ï¸ MLå·²å•Ÿç”¨ä½†æ¨¡å‹æœªè¨“ç·´ï¼Œè·³éMLåˆ†æ\\\")\\n        \\n        return enhanced_stats\\n    \\n    def get_ml_statistics(self) -> Dict:\\n        \\\"\\\"\\\"ç²å–MLçµ±è¨ˆè³‡æ–™\\\"\\\"\\\"\\n        if self.real_time_detector:\\n            return self.real_time_detector.get_statistics()\\n        return {}\\n    \\n    def configure_ml_parameters(self):\\n        \\\"\\\"\\\"é…ç½®MLåƒæ•¸\\\"\\\"\\\"\\n        if not self.ml_enabled:\\n            print(\\\"âŒ MLåµæ¸¬æœªå•Ÿç”¨\\\")\\n            return\\n        \\n        print(\\\"\\\\nğŸ”§ MLåµæ¸¬åƒæ•¸é…ç½®\\\")\\n        print(\\\"-\\\" * 30)\\n        \\n        if self.real_time_detector:\\n            current_stats = self.real_time_detector.get_statistics()\\n            print(f\\\"ç•¶å‰ç‹€æ…‹:\\\")\\n            print(f\\\"  ç›£æ§ä¸­: {current_stats['is_monitoring']}\\\")\\n            print(f\\\"  ç¸½é æ¸¬æ¬¡æ•¸: {current_stats['total_predictions']}\\\")\\n            print(f\\\"  ç¸½ç•°å¸¸æ¬¡æ•¸: {current_stats['total_anomalies']}\\\")\\n            print(f\\\"  ç•°å¸¸ç‡: {current_stats['anomaly_rate']:.2f}%\\\")\\n            \\n            # é€™è£¡å¯ä»¥æ·»åŠ äº’å‹•å¼åƒæ•¸èª¿æ•´\\n            print(f\\\"\\\\nå¯èª¿æ•´åƒæ•¸:\\\")\\n            print(f\\\"  è­¦å ±é–¾å€¼: {self.real_time_detector.alert_threshold}\\\")\\n            print(f\\\"  é€£çºŒç•°å¸¸é–¾å€¼: {self.real_time_detector.consecutive_anomalies_threshold}\\\")\\n            print(f\\\"  æ™‚é–“çª—å£: {self.real_time_detector.time_window_minutes} åˆ†é˜\\\")\\n\\n# æ¼”ç¤ºTestSelectorèˆ‡MLçš„æ•´åˆ\\nprint(\\\"\\\\nğŸ”— æ¼”ç¤ºTestSelectorèˆ‡MLç•°å¸¸åµæ¸¬çš„æ•´åˆ\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# å‰µå»ºæ•´åˆæ¼”ç¤º\\ntest_selector_ml = TestSelectorMLIntegration()\\n\\n# è¼‰å…¥æœ€ä½³æ¨¡å‹\\nif saved_models and evaluation_results:\\n    best_model_name = max(evaluation_results.items(), key=lambda x: x[1]['f1_score'])[0]\\n    best_model_path = saved_models[best_model_name]\\n    \\n    print(f\\\"è¼‰å…¥æœ€ä½³æ¨¡å‹: {best_model_name}\\\")\\n    test_selector_ml.enable_ml_detection(model_path=best_model_path, model_type=best_model_name)\\n    \\n    # å•Ÿå‹•å¯¦æ™‚ç›£æ§\\n    if test_selector_ml.real_time_detector:\\n        test_selector_ml.real_time_detector.start_monitoring()\\n        print(\\\"ğŸš€ å¯¦æ™‚ç›£æ§å·²å•Ÿå‹•\\\")\\n    \\n    # æ¨¡æ“¬RAç¨‹åºåˆ†æ\\n    print(\\\"\\\\nğŸ§ª æ¨¡æ“¬RAç¨‹åºåˆ†æ...\\\")\\n    \\n    for loop in range(1, 4):\\n        # æ¨¡æ“¬RAçµ±è¨ˆæ•¸æ“š\\n        if loop <= 2:\\n            # å‰å…©å€‹å¾ªç’°ï¼šæ­£å¸¸è¡Œç‚º\\n            mock_ra_stats = {\\n                'success_rate': np.random.normal(95, 3),\\n                'ra_initiated': np.random.randint(8, 12),\\n                'ra_succeeded': 10,\\n                'failed_attempts': 1\\n            }\\n            test_type = 'cots_only'\\n        else:\\n            # ç¬¬ä¸‰å€‹å¾ªç’°ï¼šç•°å¸¸è¡Œç‚º\\n            mock_ra_stats = {\\n                'success_rate': np.random.normal(65, 10),\\n                'ra_initiated': np.random.randint(18, 25),\\n                'ra_succeeded': 12,\\n                'failed_attempts': 8\\n            }\\n            test_type = 'standard'\\n        \\n        mock_ra_stats['success_rate'] = (mock_ra_stats['ra_succeeded'] / mock_ra_stats['ra_initiated']) * 100\\n        \\n        print(f\\\"\\\\n--- å¾ªç’° {loop} ({test_type}) ---\\\")\\n        print(f\\\"RAçµ±è¨ˆ: å•Ÿå‹•={mock_ra_stats['ra_initiated']}, æˆåŠŸ={mock_ra_stats['ra_succeeded']}, æˆåŠŸç‡={mock_ra_stats['success_rate']:.1f}%\\\")\\n        \\n        # åŸ·è¡Œå¢å¼·åˆ†æ\\n        enhanced_stats = test_selector_ml.enhanced_analyze_ra_procedure(\\n            mock_ra_stats, loop, test_type\\n        )\\n        \\n        # é¡¯ç¤ºMLåˆ†æçµæœ\\n        if 'ml_anomaly_detected' in enhanced_stats:\\n            ml_status = \\\"ç•°å¸¸\\\" if enhanced_stats['ml_anomaly_detected'] else \\\"æ­£å¸¸\\\"\\n            print(f\\\"MLåˆ†æçµæœ: {ml_status} (ä¿¡å¿ƒåº¦: {enhanced_stats.get('ml_confidence', 0):.3f})\\\")\\n        \\n        time.sleep(0.5)  # æ¨¡æ“¬è™•ç†æ™‚é–“\\n    \\n    # ç²å–æœ€çµ‚çµ±è¨ˆ\\n    time.sleep(1)  # ç­‰å¾…è™•ç†å®Œæˆ\\n    final_stats = test_selector_ml.get_ml_statistics()\\n    \\n    print(\\\"\\\\nğŸ“Š æœ€çµ‚MLçµ±è¨ˆ:\\\")\\n    for key, value in final_stats.items():\\n        print(f\\\"  {key}: {value}\\\")\\n    \\n    # åœæ­¢ç›£æ§\\n    if test_selector_ml.real_time_detector:\\n        test_selector_ml.real_time_detector.stop_monitoring()\\n        print(\\\"\\\\nâ¹ï¸ å¯¦æ™‚ç›£æ§å·²åœæ­¢\\\")\\n    \\n    # é¡¯ç¤ºé…ç½®è³‡è¨Š\\n    test_selector_ml.configure_ml_parameters()\\n    \\nelse:\\n    print(\\\"âŒ æ²’æœ‰å¯ç”¨çš„è¨“ç·´æ¨¡å‹é€²è¡Œæ•´åˆæ¼”ç¤º\\\")\\n\\nprint(\\\"\\\\nâœ… TestSelector MLæ•´åˆæ¼”ç¤ºå®Œæˆ\\\")\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2429c",
   "metadata": {},
   "source": [
    "## ç¸½çµèˆ‡ä½¿ç”¨å»ºè­°\n",
    "\n",
    "### ğŸ¯ è¨“ç·´æµç¨‹ç¸½çµ\n",
    "\n",
    "æœ¬ç­†è¨˜è©³ç´°ä»‹ç´¹äº†5Gç¶²è·¯MLç•°å¸¸åµæ¸¬æ¨¡å‹çš„å®Œæ•´è¨“ç·´æµç¨‹ï¼š\n",
    "\n",
    "1. **ç‰¹å¾µå·¥ç¨‹**: å¾RAçµ±è¨ˆã€ä¿¡è™Ÿå¼·åº¦ã€æ™‚é–“æˆ³ç­‰åŸå§‹æ•¸æ“šæå–25+å€‹ç‰¹å¾µ\n",
    "2. **æ•¸æ“šåˆæˆ**: ç”Ÿæˆæ­£å¸¸å’Œç•°å¸¸çš„åˆæˆè¨“ç·´æ•¸æ“šï¼Œæ¨¡æ“¬çœŸå¯¦ç¶²è·¯è¡Œç‚º\n",
    "3. **æ¨¡å‹è¨“ç·´**: æ”¯æ´ç„¡ç›£ç£ï¼ˆIsolation Forestã€One-Class SVMã€DBSCANï¼‰å’Œç›£ç£å­¸ç¿’ï¼ˆRandom Forestï¼‰\n",
    "4. **æ¨¡å‹è©•ä¼°**: ä½¿ç”¨æº–ç¢ºç‡ã€ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1åˆ†æ•¸ç­‰æŒ‡æ¨™è©•ä¼°æ€§èƒ½\n",
    "5. **å¯¦æ™‚åµæ¸¬**: å»ºç«‹å¤šç·šç¨‹å¯¦æ™‚ç›£æ§ç³»çµ±ï¼Œæ”¯æ´é–¾å€¼èª¿æ•´å’Œè­¦å ±æ©Ÿåˆ¶\n",
    "6. **ç³»çµ±æ•´åˆ**: å°‡MLåŠŸèƒ½ç„¡ç¸«æ•´åˆåˆ°ç¾æœ‰çš„TestSelectoræ¸¬è©¦æ¡†æ¶\n",
    "\n",
    "### ğŸ† æ¨¡å‹é¸æ“‡å»ºè­°\n",
    "\n",
    "- **Isolation Forest**: æ¨è–¦ç”¨æ–¼åˆæœŸæ¸¬è©¦å’Œéƒ¨ç½²ï¼Œç„¡éœ€æ¨™è¨˜æ•¸æ“šï¼Œæ€§èƒ½ç©©å®š\n",
    "- **Random Forest**: å¦‚æœ‰è¶³å¤ çš„æ¨™è¨˜ç•°å¸¸æ•¸æ“šï¼Œå¯æä¾›æœ€é«˜çš„æª¢æ¸¬ç²¾åº¦\n",
    "- **One-Class SVM**: é©ç”¨æ–¼å°ç•°å¸¸æ•æ„Ÿåº¦è¦æ±‚è¼ƒé«˜çš„å ´æ™¯\n",
    "- **DBSCAN**: é©ç”¨æ–¼ç™¼ç¾æœªçŸ¥çš„ç•°å¸¸æ¨¡å¼ï¼Œä½†ä¸æ”¯æ´å–®æ¨£æœ¬é æ¸¬\n",
    "\n",
    "### âš™ï¸ éƒ¨ç½²å»ºè­°\n",
    "\n",
    "1. **é–‹ç™¼éšæ®µ**: ä½¿ç”¨åˆæˆæ•¸æ“šè¨“ç·´Isolation Forestæ¨¡å‹é€²è¡Œæ¦‚å¿µé©—è­‰\n",
    "2. **æ¸¬è©¦éšæ®µ**: æ”¶é›†çœŸå¯¦çš„æ­£å¸¸é‹è¡Œæ•¸æ“šï¼Œé‡æ–°è¨“ç·´ç„¡ç›£ç£æ¨¡å‹\n",
    "3. **ç”Ÿç”¢éšæ®µ**: ç©ç´¯ç•°å¸¸æ¡ˆä¾‹å¾Œï¼Œè¨“ç·´ç›£ç£å­¸ç¿’æ¨¡å‹ä»¥æé«˜ç²¾åº¦\n",
    "4. **ç¶­è­·éšæ®µ**: å®šæœŸé‡æ–°è¨“ç·´æ¨¡å‹ï¼Œé©æ‡‰ç¶²è·¯ç’°å¢ƒè®ŠåŒ–\n",
    "\n",
    "### ğŸ”§ åƒæ•¸èª¿å„ªæŒ‡å—\n",
    "\n",
    "- **è­¦å ±é–¾å€¼**: COTS-onlyæ¸¬è©¦å»ºè­°0.8+ï¼Œæœ‰æ”»æ“Šè€…æ¸¬è©¦å»ºè­°0.6+\n",
    "- **é€£çºŒç•°å¸¸é–¾å€¼**: å»ºè­°è¨­ç‚º2-3æ¬¡ï¼Œå¹³è¡¡æ•æ„Ÿåº¦å’Œèª¤å ±ç‡\n",
    "- **æ™‚é–“çª—å£**: å»ºè­°5-10åˆ†é˜ï¼Œé©æ‡‰æ¸¬è©¦å¾ªç’°é€±æœŸ\n",
    "\n",
    "### ğŸš€ æœªä¾†æ”¹é€²æ–¹å‘\n",
    "\n",
    "1. **å¢é‡å­¸ç¿’**: å¯¦ä½œç·šä¸Šå­¸ç¿’æ©Ÿåˆ¶ï¼Œæ¨¡å‹å¯æŒçºŒé©æ‡‰æ–°çš„ç¶²è·¯ç’°å¢ƒ\n",
    "2. **å¤šæ¨¡æ…‹èåˆ**: æ•´åˆæ›´å¤šæ•¸æ“šæºï¼ˆåŠŸç‡ã€é »è­œã€æµé‡ç­‰ï¼‰æé«˜æª¢æ¸¬èƒ½åŠ›\n",
    "3. **æ·±åº¦å­¸ç¿’**: æ¢ç´¢LSTMã€GRUç­‰æ™‚åºæ¨¡å‹æ•æ‰æ›´è¤‡é›œçš„ç•°å¸¸æ¨¡å¼\n",
    "4. **è¯é‚¦å­¸ç¿’**: åœ¨å¤šå€‹åŸºç«™é–“å…±äº«å­¸ç¿’æˆæœï¼ŒåŒæ™‚ä¿è­·éš±ç§\n",
    "5. **å¯è§£é‡‹æ€§**: å¢åŠ æ¨¡å‹æ±ºç­–çš„å¯è§£é‡‹æ€§ï¼Œå¹«åŠ©å·¥ç¨‹å¸«ç†è§£ç•°å¸¸åŸå› \n",
    "\n",
    "### ğŸ“‹ æª¢æŸ¥æ¸…å–®\n",
    "\n",
    "éƒ¨ç½²å‰è«‹ç¢ºèªï¼š\n",
    "- [ ] æ¨¡å‹å·²åœ¨è¶³å¤ çš„è¨“ç·´æ•¸æ“šä¸Šè¨“ç·´\n",
    "- [ ] è©•ä¼°æŒ‡æ¨™æ»¿è¶³æ¥­å‹™éœ€æ±‚ï¼ˆå»ºè­°F1 > 0.8ï¼‰\n",
    "- [ ] è­¦å ±é–¾å€¼å·²æ ¹æ“šå¯¦éš›ç’°å¢ƒèª¿æ•´\n",
    "- [ ] å¯¦æ™‚ç›£æ§ç³»çµ±é‹è¡Œç©©å®š\n",
    "- [ ] ç•°å¸¸è™•ç†å›èª¿å‡½æ•¸å·²æ­£ç¢ºé…ç½®\n",
    "- [ ] æ¨¡å‹æª”æ¡ˆå·²å‚™ä»½ä¸¦å¯å¿«é€Ÿè¼‰å…¥\n",
    "\n",
    "é€éæœ¬ç­†è¨˜çš„æŒ‡å°ï¼Œæ‚¨æ‡‰è©²èƒ½å¤ æˆåŠŸå»ºç«‹å’Œéƒ¨ç½²ä¸€å¥—å®Œæ•´çš„5Gç¶²è·¯MLç•°å¸¸åµæ¸¬ç³»çµ±ã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
